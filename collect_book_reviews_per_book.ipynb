{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n# import pandas as pd\nfrom datetime import datetime\nimport time\nimport random\n\ndef search_book_reviews(book_name, author):\n    # Combine book name and author for search query\n    search_query = f\"{book_name} {author} book review\"\n    \n    # Perform Google search\n    url = f\"https://www.google.com/search?q={search_query.replace(' ', '+')}\"\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n    \n    response = requests.get(url, headers=headers)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Extract search results\n    search_results = soup.find_all('div', class_='g')\n    \n    reviews = []\n    \n    for result in search_results:\n        link = result.find('a')\n        if link:\n            review_url = link['href']\n            \n            # Skip non-http links and known non-review sites\n            if not review_url.startswith('http') or any(site in review_url for site in ['google.com', 'wikipedia.org']):\n                continue\n            \n            try:\n                # Fetch the review page\n                review_response = requests.get(review_url, headers=headers, timeout=10)\n                review_soup = BeautifulSoup(review_response.text, 'html.parser')\n                \n                # Extract review text (this is a simplistic approach and may need refinement)\n                review_text = ' '.join([p.text for p in review_soup.find_all('p')])\n                \n                # Extract date (this is a placeholder, as date formats vary widely)\n                date = review_soup.find('time')\n                review_date = date['datetime'] if date else 'Unknown'\n                \n                reviews.append({\n                    'review_text': review_text[:500],  # Limit to first 500 characters\n                    'review_date': review_date,\n                    'review_website': review_url\n                })\n                \n            except Exception as e:\n                print(f\"Error processing {review_url}: {str(e)}\")\n            \n            # Be polite to servers\n            time.sleep(random.uniform(1, 3))\n    \n    # Create DataFrame\n    df = pd.DataFrame(reviews)\n    \n    return df\n\n# Example usage\nbook_name = \"To Kill a Mockingbird\"\nauthor = \"Harper Lee\"\n\ndf = search_book_reviews(book_name, author)\n\n# Save to CSV\ndf.to_csv(f\"{book_name.replace(' ', '_')}_reviews.csv\", index=False)\nprint(f\"Reviews saved to {book_name.replace(' ', '_')}_reviews.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}