{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9243923,"sourceType":"datasetVersion","datasetId":5562194}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nigamshitij/parse-book-reviews-using-duckduckgo?scriptVersionId=195259701\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-04T04:16:53.093548Z","iopub.execute_input":"2024-09-04T04:16:53.093971Z","iopub.status.idle":"2024-09-04T04:16:54.295331Z","shell.execute_reply.started":"2024-09-04T04:16:53.093936Z","shell.execute_reply":"2024-09-04T04:16:54.293945Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/book-review-samples/goodreads_fantasy.csv\n/kaggle/input/book-review-samples/goodreads_all_genres.csv\n/kaggle/input/book-review-samples/Darth_Plagueis_reviews(4).csv\n/kaggle/input/book-review-samples/goodreads_all_genres_final.csv\n/kaggle/input/book-review-samples/Dune_Book_1_reviews.csv\n/kaggle/input/book-review-samples/Dune_Book_1_reviews(4).csv\n/kaggle/input/book-review-samples/goodreads_genres_complete.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install duckduckgo-search","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:16:54.297843Z","iopub.execute_input":"2024-09-04T04:16:54.298368Z","iopub.status.idle":"2024-09-04T04:17:12.229736Z","shell.execute_reply.started":"2024-09-04T04:16:54.298327Z","shell.execute_reply":"2024-09-04T04:17:12.228142Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting duckduckgo-search\n  Downloading duckduckgo_search-6.2.11-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: click>=8.1.7 in /opt/conda/lib/python3.10/site-packages (from duckduckgo-search) (8.1.7)\nCollecting primp>=0.6.1 (from duckduckgo-search)\n  Downloading primp-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nDownloading duckduckgo_search-6.2.11-py3-none-any.whl (27 kB)\nDownloading primp-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: primp, duckduckgo-search\nSuccessfully installed duckduckgo-search-6.2.11 primp-0.6.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nimport time\nimport random\nimport re\nimport html\nfrom duckduckgo_search import DDGS\nimport glob\nimport csv\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:12.231563Z","iopub.execute_input":"2024-09-04T04:17:12.231994Z","iopub.status.idle":"2024-09-04T04:17:12.586013Z","shell.execute_reply.started":"2024-09-04T04:17:12.231957Z","shell.execute_reply":"2024-09-04T04:17:12.584857Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# boilerplate removal\n\ndef remove_boilerplate(text):\n    boilerplate = ['cookie policy', 'privacy policy', 'terms of service', 'all rights reserved', '\\n']\n    for phrase in boilerplate:\n        text = re.sub(r'(?i)' + re.escape(phrase) + r'.*', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:12.589038Z","iopub.execute_input":"2024-09-04T04:17:12.589801Z","iopub.status.idle":"2024-09-04T04:17:12.597413Z","shell.execute_reply.started":"2024-09-04T04:17:12.589757Z","shell.execute_reply":"2024-09-04T04:17:12.596108Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# comment truncation (20% threshold default)\n\ndef truncate_at_comments(review_text, threshold_percentage=20, min_review_length=100):\n    comment_identifiers = [\n        'Comments', 'Leave a comment', 'Reader comments', \n        'What do you think?', 'Join the discussion', 'Add a comment',\n        'Post a comment', 'Write a comment', 'Show comments'\n    ]\n    \n    lower_text = review_text.lower()\n    text_length = len(lower_text)\n    threshold = max(int(text_length * (threshold_percentage / 100)), min_review_length)\n\n    # Check for comment identifiers\n    for identifier in comment_identifiers:\n        index = lower_text.find(identifier.lower())\n        if index != -1 and index > threshold:\n            return review_text[:index].strip()\n    \n    # If no identifiers found, try to detect comment-like structures\n    paragraphs = review_text.split('\\n\\n')\n    filtered_paragraphs = []\n    \n    for paragraph in paragraphs:\n        # Skip short paragraphs that might be comments\n        if len(paragraph) < 50:\n            continue\n        \n        # Skip paragraphs that start with common comment patterns\n        if re.match(r'^(Posted by|From|User|Anonymous|[\\d/]+:)', paragraph.strip()):\n            continue\n        \n        filtered_paragraphs.append(paragraph)\n    \n    # If we've removed some paragraphs, join the remaining ones\n    if len(filtered_paragraphs) < len(paragraphs):\n        return '\\n\\n'.join(filtered_paragraphs).strip()\n    \n    # If we haven't removed any paragraphs, return at least the first part of the text\n    return review_text[:max(threshold, len(review_text) // 2)].strip()","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:12.59888Z","iopub.execute_input":"2024-09-04T04:17:12.599299Z","iopub.status.idle":"2024-09-04T04:17:12.611753Z","shell.execute_reply.started":"2024-09-04T04:17:12.599267Z","shell.execute_reply":"2024-09-04T04:17:12.610474Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# remove comment classes\n\ndef remove_comments(review_soup):\n    # Common class names for comment sections\n    comment_classes = ['comment', 'comments-list', 'comments-area', 'comments', 'comment-section', 'user-comments', 'disqus_thread']\n    \n    for class_name in comment_classes:\n        comment_section = review_soup.find('div', class_=class_name)\n        if comment_section:\n            comment_section.decompose()  # This removes the element from the soup\n    \n    return review_soup","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:12.61336Z","iopub.execute_input":"2024-09-04T04:17:12.613754Z","iopub.status.idle":"2024-09-04T04:17:12.627359Z","shell.execute_reply.started":"2024-09-04T04:17:12.613724Z","shell.execute_reply":"2024-09-04T04:17:12.62633Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# paragraph scoring for review content\n\ndef score_paragraph(paragraph):\n    review_keywords = ['review', 'book', 'read', 'author', 'story', 'character', 'plot', 'recommend']\n    return sum(keyword in paragraph.lower() for keyword in review_keywords)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:12.628914Z","iopub.execute_input":"2024-09-04T04:17:12.629374Z","iopub.status.idle":"2024-09-04T04:17:12.643119Z","shell.execute_reply.started":"2024-09-04T04:17:12.629335Z","shell.execute_reply":"2024-09-04T04:17:12.641952Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# add retries with timeouts selectively\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nfrom urllib3.exceptions import ReadTimeoutError\nfrom bs4 import BeautifulSoup\nimport time\nimport random\n\ndef create_session_with_retries():\n    session = requests.Session()\n    retries = Retry(total=5, \n                    backoff_factor=1, \n                    status_forcelist=[429, 500, 502, 503, 504],\n                    allowed_methods=[\"HEAD\", \"GET\", \"OPTIONS\"])\n    adapter = HTTPAdapter(max_retries=retries)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n    return session","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:12.64443Z","iopub.execute_input":"2024-09-04T04:17:12.644787Z","iopub.status.idle":"2024-09-04T04:17:12.655305Z","shell.execute_reply.started":"2024-09-04T04:17:12.644727Z","shell.execute_reply":"2024-09-04T04:17:12.654243Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"exceptions = ('google.', 'goodreads.com', 'amazon.', 'reddit.com', 'thestorygraph', 'youtube','.tv', 'barnesandnoble.com','wikipedia.','quora.com','sparknotes.com', 'grimdarkmagazine.', 'washingtonpost.')\n\ndef process_review_url(review_url, headers):\n    reviews = []\n    # Skip known non-review sites\n    # if any(site in review_url for site in ['google.com', 'wikipedia.org']):\n    if any(site in review_url.lower() for site in exceptions):\n        return reviews\n    \n    session = create_session_with_retries()\n    \n    try:\n        # Fetch the review page\n        # review_response = requests.get(review_url, headers=headers, timeout=10)\n        review_response = session.get(review_url, headers=headers, timeout=10)  # Increased timeout to 20 seconds\n        review_soup = BeautifulSoup(review_response.text, 'html.parser')\n        \n        # Remove Comments\n        review_soup = remove_comments(review_soup)\n        \n        # Review elements\n        review_elements = review_soup.find_all(['main','p'])\n        scored_paragraphs = [(elem, score_paragraph(elem.text)) for elem in review_elements]\n        \n        # Extract review text\n        review_paragraphs = [elem.text for elem, score in sorted(scored_paragraphs, key=lambda x: x[1], reverse=True)[:3]]\n        \n        # Search for relevant text\n        review_text = ' '.join(review_paragraphs)\n        \n        # Remove boilerplate\n        review_text = review_text.replace('\\n', ' ')\n        review_text = remove_boilerplate(review_text)\n        \n        # Truncate at Comments\n        review_text = truncate_at_comments(review_text)\n        \n        review_date = 'Unknown'\n        \n        reviews.append({\n            'review_text': review_text[:5000],  # Limit to first 5000 characters\n            'review_date': review_date,\n            'review_website': review_url\n        })\n        \n    except ReadTimeoutError as e:\n        print(f\"Read timeout error for {review_url}: {str(e)}\")\n    except requests.exceptions.RequestException as e:\n        print(f\"Error processing {review_url}: {str(e)}\")\n    except Exception as e:\n        print(f\"Unexpected error processing {review_url}: {str(e)}\")\n    \n    # Be polite to servers\n    time.sleep(random.uniform(1, 3))\n    \n    return reviews","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:12.657092Z","iopub.execute_input":"2024-09-04T04:17:12.657479Z","iopub.status.idle":"2024-09-04T04:17:12.672475Z","shell.execute_reply.started":"2024-09-04T04:17:12.657447Z","shell.execute_reply":"2024-09-04T04:17:12.671462Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# search_book_review test\n\ndef search_book_reviews(book_name, author):\n    # Combine book name and author for search query\n    search_query = f\"{book_name} {author} book review\"\n    \n    # print(f\"Searching for: {search_query}\")\n    \n    reviews = []\n    \n    # Define headers\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n    \n    # Use DDGS for searching\n    with DDGS() as ddgs:\n        results = ddgs.text(search_query, max_results=10)  # Adjust max_results as needed\n        \n        for result in results:\n            review_url = result['href']\n            reviews.extend(process_review_url(review_url, headers))\n    \n    # Create DataFrame\n    df = pd.DataFrame(reviews)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:12.676387Z","iopub.execute_input":"2024-09-04T04:17:12.676764Z","iopub.status.idle":"2024-09-04T04:17:12.688287Z","shell.execute_reply.started":"2024-09-04T04:17:12.676726Z","shell.execute_reply":"2024-09-04T04:17:12.687141Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# # Example usage\n# book_name = \"Darth Plagueis\"\n# author = \"James Luceno\"\n\n# df = search_book_reviews(book_name, author)\n\n# # Save to CSV\n# df.to_csv(f\"{book_name.replace(' ', '_')}_reviews.csv\", index=False)\n# print(f\"Reviews saved to {book_name.replace(' ', '_')}_reviews.csv\")\n\n# df.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:12.690106Z","iopub.execute_input":"2024-09-04T04:17:12.69055Z","iopub.status.idle":"2024-09-04T04:17:12.705132Z","shell.execute_reply.started":"2024-09-04T04:17:12.690508Z","shell.execute_reply":"2024-09-04T04:17:12.703923Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# input_csv = '/kaggle/input/book-review-samples/goodreads_all_genres_final.csv'\n# output_csv = 'book_reviews.csv'\n\ninput_csv = '/kaggle/input/book-review-samples/goodreads_all_genres_final.csv'\noutput_directory = '/kaggle/working/'\noutput_csv = 'all_book_reviews.csv'\n\ndf = pd.read_csv(input_csv)\ndf_unique = df.drop_duplicates(subset=['Title', 'Authors'], keep='first')\nnum_duplicates = len(df) - len(df_unique)\nnum_total = len(df)\nprint(f\"Removed {num_duplicates} duplicate entries out of {num_total}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:12.709474Z","iopub.execute_input":"2024-09-04T04:17:12.710322Z","iopub.status.idle":"2024-09-04T04:17:12.999271Z","shell.execute_reply.started":"2024-09-04T04:17:12.710283Z","shell.execute_reply":"2024-09-04T04:17:12.997927Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Removed 0 duplicate entries out of 35597\n","output_type":"stream"}]},{"cell_type":"code","source":"def process_book_list_old(input_csv, output_csv):\n    # Read the input CSV\n    df = pd.read_csv(input_csv)\n    \n    # remove duplicates\n    df_unique = df.drop_duplicates(subset=['Title', 'Authors'], keep='first')\n    \n    # Print information about removed duplicates\n    num_duplicates = len(df) - len(df_unique)\n    num_total = len(df)\n    print(f\"Removed {num_duplicates} duplicate entries out of {num_total}\")\n    \n    all_reviews = []\n    \n    # Iterate through each row in the dataframe\n    for index, row in tqdm(df_unique.iterrows(), total=df_unique.shape[0], desc=\"Processing books\"):\n        title = row['Title']\n        authors = row['Authors']\n        \n        try:\n            # Get reviews for this book\n            reviews_df = search_book_reviews(title, authors)\n            \n            # Add book information to each review\n            reviews_df['Title'] = title\n            reviews_df['Authors'] = authors\n            reviews_df['Avg Ratings'] = row['Avg Ratings']\n            reviews_df['Rating'] = row['Rating']\n            reviews_df['Published_year'] = row['Published_year']\n            \n            all_reviews.append(reviews_df)\n        \n        except Exception as e:\n            print(f\"Error processing {title} by {authors}: {str(e)}\")\n    \n    # Combine all reviews into a single dataframe\n    if all_reviews:\n        final_df = pd.concat(all_reviews, ignore_index=True)\n        \n        # Save to CSV\n        final_df.to_csv(output_csv, index=False, quoting=csv.QUOTE_ALL)\n        print(f\"Reviews saved to {output_csv}\")\n    else:\n        print(\"No reviews were collected.\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:13.00086Z","iopub.execute_input":"2024-09-04T04:17:13.001348Z","iopub.status.idle":"2024-09-04T04:17:13.014519Z","shell.execute_reply.started":"2024-09-04T04:17:13.001301Z","shell.execute_reply":"2024-09-04T04:17:13.013078Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def process_book_list_old_2(input_csv, output_csv):\n    # Read the input CSV\n    df = pd.read_csv(input_csv)\n    \n    # remove duplicates\n    df_unique = df.drop_duplicates(subset=['Title', 'Authors'], keep='first')\n    \n    # Print information about removed duplicates\n    num_duplicates = len(df) - len(df_unique)\n    num_total = len(df)\n    print(f\"Removed {num_duplicates} duplicate entries out of {num_total}\")\n    \n    all_reviews = []\n    batch_size = 10\n    batch_count = 0\n    \n    # Iterate through each row in the dataframe\n    for index, row in tqdm(df_unique.iterrows(), total=df_unique.shape[0], desc=\"Processing books\"):\n        title = row['Title']\n        authors = row['Authors']\n        \n        try:\n            # Get reviews for this book\n            reviews_df = search_book_reviews(title, authors)\n            \n            # Add book information to each review\n            reviews_df['Title'] = title\n            reviews_df['Authors'] = authors\n            reviews_df['Avg Ratings'] = row['Avg Ratings']\n            reviews_df['Rating'] = row['Rating']\n            reviews_df['Published_year'] = row['Published_year']\n            \n            all_reviews.append(reviews_df)\n        \n        except Exception as e:\n            print(f\"Error processing {title} by {authors}: {str(e)}\")\n        \n        # Save batch to CSV every 10 books\n        if len(all_reviews) == batch_size:\n            batch_df = pd.concat(all_reviews, ignore_index=True)\n            batch_filename = f\"/kaggle/working/batch_{batch_count}.csv\"\n            batch_df.to_csv(\n                batch_filename, \n                index=False, \n                quoting=csv.QUOTE_ALL,\n                escapechar='\\\\',\n                doublequote=False\n            )\n            print(f\"Batch {batch_count} saved to {batch_filename}\")\n            all_reviews = []\n            batch_count += 1\n    \n    # Save any remaining reviews\n    if all_reviews:\n        batch_df = pd.concat(all_reviews, ignore_index=True)\n        batch_filename = f\"/kaggle/working/batch_{batch_count}.csv\"\n        batch_df.to_csv(batch_filename, index=False, quoting=csv.QUOTE_ALL)\n        print(f\"Final batch saved to {batch_filename}\")\n    \n    # Combine all batches\n    all_files = glob.glob(\"/kaggle/working/batch_*.csv\")\n    combined_df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n    \n    # Save combined results\n    combined_df.to_csv(\n        output_csv, \n        index=False, \n        quoting=csv.QUOTE_ALL,\n        escapechar='\\\\',\n        doublequote=False\n    )\n    print(f\"All reviews combined and saved to {output_csv}\")\n    \n    # Optionally, remove batch files\n    for f in all_files:\n        os.remove(f)\n    print(\"Batch files removed\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:13.016288Z","iopub.execute_input":"2024-09-04T04:17:13.016686Z","iopub.status.idle":"2024-09-04T04:17:13.035124Z","shell.execute_reply.started":"2024-09-04T04:17:13.016639Z","shell.execute_reply":"2024-09-04T04:17:13.033971Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def process_book_list(input_csv, output_csv):\n    # Read the input CSV\n    df = pd.read_csv(input_csv)\n    \n    # Remove duplicates\n    df_unique = df.drop_duplicates(subset=['Title', 'Authors'], keep='first')\n    \n    # Print information about removed duplicates\n    num_duplicates = len(df) - len(df_unique)\n    num_total = len(df)\n    print(f\"Removed {num_duplicates} duplicate entries out of {num_total}\")\n    \n    # Find the last processed batch\n    existing_batches = glob.glob(\"/kaggle/working/batch_*.csv\")\n    if existing_batches:\n        last_batch = max(existing_batches, key=os.path.getctime)\n        last_batch_number = int(last_batch.split('_')[1].split('.')[0])\n        start_index = (last_batch_number + 1) * 10\n    else:\n        start_index = 0\n    \n    print(f\"Resuming processing from index {start_index}\")\n    \n    batch_size = 10\n    batch_count = start_index // 10\n    \n    # Iterate through each row in the dataframe, starting from start_index\n    for index in tqdm(range(start_index, len(df_unique)), total=len(df_unique)-start_index, desc=\"Processing books\"):\n        row = df_unique.iloc[index]\n        title = row['Title']\n        authors = row['Authors']\n        \n        try:\n            # Get reviews for this book\n            reviews_df = search_book_reviews(title, authors)\n            \n            # Add book information to each review\n            reviews_df['Title'] = title\n            reviews_df['Authors'] = authors\n            reviews_df['Avg Ratings'] = row['Avg Ratings']\n            reviews_df['Rating'] = row['Rating']\n            reviews_df['Published_year'] = row['Published_year']\n            \n            # Save batch to CSV every 10 books\n            if (index + 1) % batch_size == 0 or index == len(df_unique) - 1:\n                batch_filename = f\"/kaggle/working/batch_{batch_count}.csv\"\n                reviews_df.to_csv(batch_filename, index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\', doublequote=False)\n                print(f\"Batch {batch_count} saved to {batch_filename}\")\n                batch_count += 1\n        \n        except Exception as e:\n            print(f\"Error processing {title} by {authors}: {str(e)}\")\n    \n    # Combine all batches\n    all_files = glob.glob(\"/kaggle/working/batch_*.csv\")\n    combined_df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n    \n    # Save combined results\n    combined_df.to_csv(output_csv, index=False, quoting=csv.QUOTE_ALL, escapechar='\\\\', doublequote=False)\n    print(f\"All reviews combined and saved to {output_csv}\")\n    \n    # Optionally, remove batch files\n    for f in all_files:\n        os.remove(f)\n    print(\"Batch files removed\")","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:13.036782Z","iopub.execute_input":"2024-09-04T04:17:13.03725Z","iopub.status.idle":"2024-09-04T04:17:13.05431Z","shell.execute_reply.started":"2024-09-04T04:17:13.037205Z","shell.execute_reply":"2024-09-04T04:17:13.053041Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Usage\nprocess_book_list(input_csv, output_csv)","metadata":{"execution":{"iopub.status.busy":"2024-09-04T04:17:13.05594Z","iopub.execute_input":"2024-09-04T04:17:13.056703Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Removed 0 duplicate entries out of 35597\nResuming processing from index 1560\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 10/34037 [01:47<108:19:53, 11.46s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 156 saved to /kaggle/working/batch_156.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 12/34037 [02:13<121:12:57, 12.83s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.kirkusreviews.com/book-reviews/kim-mclarin/james-baldwins-another-country/: HTTPSConnectionPool(host='www.kirkusreviews.com', port=443): Max retries exceeded with url: /book-reviews/kim-mclarin/james-baldwins-another-country/ (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.kirkusreviews.com', port=443): Read timed out. (read timeout=10)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 18/34037 [05:29<203:35:21, 21.54s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.kirandellimore.com/book-review-notes-of-a-native-son-james-baldwin/: HTTPSConnectionPool(host='www.kirandellimore.com', port=443): Max retries exceeded with url: /book-review-notes-of-a-native-son-james-baldwin/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 19/34037 [06:20<286:17:00, 30.30s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://paperbacksocial.com/2021/01/30/the-women-of-brewster-place-by-gloria-naylor-review/: HTTPSConnectionPool(host='paperbacksocial.com', port=443): Max retries exceeded with url: /2021/01/30/the-women-of-brewster-place-by-gloria-naylor-review/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 20/34037 [07:11<347:17:10, 36.75s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 157 saved to /kaggle/working/batch_157.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 27/34037 [09:47<232:19:55, 24.59s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.beverlyjenkins.net/books/old-west-series/forbidden/: HTTPSConnectionPool(host='www.beverlyjenkins.net', port=443): Max retries exceeded with url: /books/old-west-series/forbidden/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 30/34037 [11:08<221:53:28, 23.49s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 158 saved to /kaggle/working/batch_158.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 40/34037 [12:43<94:53:11, 10.05s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 159 saved to /kaggle/working/batch_159.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 50/34037 [15:28<215:34:42, 22.83s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 160 saved to /kaggle/working/batch_160.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 52/34037 [15:50<161:31:13, 17.11s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.beverlyjenkins.net/books/destiny-trilogy/destinys-embrace/: HTTPSConnectionPool(host='www.beverlyjenkins.net', port=443): Max retries exceeded with url: /books/destiny-trilogy/destinys-embrace/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 60/34037 [17:48<130:31:37, 13.83s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 161 saved to /kaggle/working/batch_161.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 68/34037 [19:08<93:04:29,  9.86s/it] ","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.beverlyjenkins.net/books/women-who-dare-series/rebel/: HTTPSConnectionPool(host='www.beverlyjenkins.net', port=443): Max retries exceeded with url: /books/women-who-dare-series/rebel/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\nError processing https://www.beverlyjenkins.net/books/women-who-dare-series/: HTTPSConnectionPool(host='www.beverlyjenkins.net', port=443): Max retries exceeded with url: /books/women-who-dare-series/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 70/34037 [20:26<204:48:18, 21.71s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 162 saved to /kaggle/working/batch_162.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 76/34037 [21:32<100:51:47, 10.69s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.readinggroupguides.com/reviews/the-secret-lives-of-baba-segis-wives/guide: HTTPSConnectionPool(host='www.readinggroupguides.com', port=443): Max retries exceeded with url: /reviews/the-secret-lives-of-baba-segis-wives/guide (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.readinggroupguides.com', port=443): Read timed out. (read timeout=10)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 80/34037 [23:57<200:22:42, 21.24s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 163 saved to /kaggle/working/batch_163.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 87/34037 [25:13<113:55:36, 12.08s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://thebibliophage.com/book-review-freshwater-akwaeke-emezi/: HTTPSConnectionPool(host='thebibliophage.com', port=443): Max retries exceeded with url: /book-review-freshwater-akwaeke-emezi/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 90/34037 [26:59<224:41:15, 23.83s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 164 saved to /kaggle/working/batch_164.csv\nError processing https://www.kirandellimore.com/book-review-julys-people-nadine-gordimer/: HTTPSConnectionPool(host='www.kirandellimore.com', port=443): Max retries exceeded with url: /book-review-julys-people-nadine-gordimer/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 96/34037 [28:56<154:08:35, 16.35s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://paperbacksocial.com/2022/01/19/woman-at-point-zero-by-nawal-el-saadawi-book-review/: HTTPSConnectionPool(host='paperbacksocial.com', port=443): Max retries exceeded with url: /2022/01/19/woman-at-point-zero-by-nawal-el-saadawi-book-review/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 100/34037 [30:20<155:38:20, 16.51s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 165 saved to /kaggle/working/batch_165.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 106/34037 [32:02<163:15:32, 17.32s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://novelfables.com/artificial-condition: HTTPSConnectionPool(host='novelfables.com', port=443): Max retries exceeded with url: /artificial-condition (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7a03e3dabdf0>: Failed to resolve 'novelfables.com' ([Errno -5] No address associated with hostname)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 110/34037 [33:43<188:28:48, 20.00s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 166 saved to /kaggle/working/batch_166.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 117/34037 [35:21<137:49:40, 14.63s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://diversebooks.org/qa-with-mark-oshiro-the-insiders/: HTTPSConnectionPool(host='diversebooks.org', port=443): Max retries exceeded with url: /qa-with-mark-oshiro-the-insiders/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 120/34037 [36:42<197:38:09, 20.98s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 167 saved to /kaggle/working/batch_167.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 126/34037 [38:24<154:40:32, 16.42s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://candidceillie.com/review-baker-thief-claudie-arseneault/: HTTPSConnectionPool(host='candidceillie.com', port=443): Max retries exceeded with url: /review-baker-thief-claudie-arseneault/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 130/34037 [39:52<161:29:42, 17.15s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 168 saved to /kaggle/working/batch_168.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 140/34037 [41:48<110:23:58, 11.72s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 169 saved to /kaggle/working/batch_169.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 150/34037 [44:25<191:07:31, 20.30s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 170 saved to /kaggle/working/batch_170.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 160/34037 [45:58<90:20:10,  9.60s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 171 saved to /kaggle/working/batch_171.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 170/34037 [47:36<99:31:52, 10.58s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 172 saved to /kaggle/working/batch_172.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 180/34037 [48:53<89:01:21,  9.47s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 173 saved to /kaggle/working/batch_173.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 190/34037 [50:56<107:28:50, 11.43s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 174 saved to /kaggle/working/batch_174.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 193/34037 [51:04<50:27:05,  5.37s/it] ","output_type":"stream"},{"name":"stdout","text":"Error processing For the Love of Soil: Strategies to Regenerate Our Food Production Systems (Kindle Edition) by Nicole Masters: https://links.duckduckgo.com/d.js?q=For+the+Love+of+Soil%3A+Strategies+to+Regenerate+Our+Food+Production+Systems+%28Kindle+Edition%29+Nicole+Masters+book+review&kl=wt-wt&l=wt-wt&p=&s=0&df=&vqd=4-102985106534766914171393745815591343429&bing_market=wt-WT&ex=-1 202 Ratelimit\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 200/34037 [52:03<94:56:20, 10.10s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 175 saved to /kaggle/working/batch_175.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 210/34037 [54:11<127:26:50, 13.56s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 176 saved to /kaggle/working/batch_176.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 216/34037 [54:51<58:43:03,  6.25s/it] ","output_type":"stream"},{"name":"stdout","text":"Error processing Wings of the Luftwaffe: Flying German Aircraft of the Second World War (Paperback) by Eric M.  Brown: https://links.duckduckgo.com/d.js?q=Wings+of+the+Luftwaffe%3A+Flying+German+Aircraft+of+the+Second+World+War+%28Paperback%29+Eric+M.++Brown+book+review&kl=wt-wt&l=wt-wt&p=&s=0&df=&vqd=4-177837932682667247505135637314895882528&bing_market=wt-WT&ex=-1 202 Ratelimit\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 220/34037 [55:25<85:18:07,  9.08s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 177 saved to /kaggle/working/batch_177.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 230/34037 [57:19<120:19:21, 12.81s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 178 saved to /kaggle/working/batch_178.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 231/34037 [57:37<134:22:12, 14.31s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.cybermodeler.com/hobby/ref/osp/book_osp_xp02.shtml: HTTPSConnectionPool(host='www.cybermodeler.com', port=443): Max retries exceeded with url: /hobby/ref/osp/book_osp_xp02.shtml (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 237/34037 [59:25<99:16:05, 10.57s/it] ","output_type":"stream"},{"name":"stdout","text":"Error processing Luftwaffe Fighter Aces: The Jagdflieger and Their Combat Tactics and Techniques (Paperback) by Mike Spick: https://links.duckduckgo.com/d.js?q=Luftwaffe+Fighter+Aces%3A+The+Jagdflieger+and+Their+Combat+Tactics+and+Techniques+%28Paperback%29+Mike+Spick+book+review&kl=wt-wt&l=wt-wt&p=&s=0&df=&vqd=4-106257053064897860316510485128978703256&bing_market=wt-WT&ex=-1 202 Ratelimit\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 238/34037 [59:25<70:28:14,  7.51s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing Horrido! Fighter Aces Of The Luftwaffe (Mass Market Paperback) by Raymond F. Toliver: https://links.duckduckgo.com/d.js?q=Horrido%21+Fighter+Aces+Of+The+Luftwaffe+%28Mass+Market+Paperback%29+Raymond+F.+Toliver+book+review&kl=wt-wt&l=wt-wt&p=&s=0&df=&vqd=4-335900581739264176511763004637029815360&bing_market=wt-WT&ex=-1 202 Ratelimit\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 240/34037 [59:50<99:05:50, 10.56s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 179 saved to /kaggle/working/batch_179.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 241/34037 [59:51<70:12:44,  7.48s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing Graphic War: The Secret Aviation Drawings and Illustrations of World War II (Hardcover) by Donald Nijboer: https://links.duckduckgo.com/d.js?q=Graphic+War%3A+The+Secret+Aviation+Drawings+and+Illustrations+of+World+War+II+%28Hardcover%29+Donald+Nijboer+book+review&kl=wt-wt&l=wt-wt&p=&s=0&df=&vqd=4-119610666732831612494744400935419261271&bing_market=wt-WT&ex=-1 202 Ratelimit\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 250/34037 [1:01:21<98:46:15, 10.52s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 180 saved to /kaggle/working/batch_180.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 259/34037 [1:03:37<145:01:20, 15.46s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://escapetotheseventies.com/70s-films/the-incredible-melting-man/: HTTPSConnectionPool(host='escapetotheseventies.com', port=443): Max retries exceeded with url: /70s-films/the-incredible-melting-man/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate (_ssl.c:1007)')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 260/34037 [1:04:29<248:27:00, 26.48s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 181 saved to /kaggle/working/batch_181.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 270/34037 [1:07:47<184:25:41, 19.66s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 182 saved to /kaggle/working/batch_182.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 280/34037 [1:11:08<138:59:24, 14.82s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 183 saved to /kaggle/working/batch_183.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 283/34037 [1:11:37<100:46:28, 10.75s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.flipkart.com/physics-module-v-optics-modern/p/itmetpv2fuzsczvh?pid=9789352605279&lid=LSTBOK9789352605279ZLEZMR&marketplace=FLIPKART: HTTPSConnectionPool(host='www.flipkart.com', port=443): Max retries exceeded with url: /physics-module-v-optics-modern/p/itmetpv2fuzsczvh?pid=9789352605279&lid=LSTBOK9789352605279ZLEZMR&marketplace=FLIPKART (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.flipkart.com', port=443): Read timed out. (read timeout=10)\"))\nError processing https://www.flipkart.com/optics-modern-physics-neet-module-v/p/itmf5rthz3hgehcm?pid=9789387432536&marketplace=FLIPKART: HTTPSConnectionPool(host='www.flipkart.com', port=443): Max retries exceeded with url: /optics-modern-physics-neet-module-v/p/itmf5rthz3hgehcm?pid=9789387432536&marketplace=FLIPKART (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.flipkart.com', port=443): Read timed out. (read timeout=10)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 290/34037 [1:16:11<187:42:03, 20.02s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 184 saved to /kaggle/working/batch_184.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 292/34037 [1:16:41<159:36:33, 17.03s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://seedstosuccess.com/know/the-two-bite-club-by-team-nutrition-2012/: HTTPSConnectionPool(host='seedstosuccess.com', port=443): Max retries exceeded with url: /know/the-two-bite-club-by-team-nutrition-2012/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 293/34037 [1:17:32<254:26:21, 27.15s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://blackmaskmagazine.com/blog/w-t-ballard-an-interview/: HTTPSConnectionPool(host='blackmaskmagazine.com', port=443): Max retries exceeded with url: /blog/w-t-ballard-an-interview/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 300/34037 [1:19:58<163:49:17, 17.48s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 185 saved to /kaggle/working/batch_185.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 310/34037 [1:21:43<101:52:43, 10.87s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 186 saved to /kaggle/working/batch_186.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 319/34037 [1:23:02<76:52:59,  8.21s/it] ","output_type":"stream"},{"name":"stdout","text":"Error processing https://stuffedpuffin.eu/2021/12/16/book-review-terminal-world-by-alastair-reynolds/: HTTPSConnectionPool(host='stuffedpuffin.eu', port=443): Max retries exceeded with url: /2021/12/16/book-review-terminal-world-by-alastair-reynolds/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 320/34037 [1:23:52<194:31:37, 20.77s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 187 saved to /kaggle/working/batch_187.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 327/34037 [1:25:19<89:48:56,  9.59s/it] ","output_type":"stream"},{"name":"stdout","text":"Error processing The Golden Age of the Great Passenger Airships: Graf Zeppelin and Hindenburg (Paperback) by Harold G. Dick: https://links.duckduckgo.com/d.js?q=The+Golden+Age+of+the+Great+Passenger+Airships%3A+Graf+Zeppelin+and+Hindenburg+%28Paperback%29+Harold+G.+Dick+book+review&kl=wt-wt&l=wt-wt&p=&s=0&df=&vqd=4-255965238779898816532550939081565538751&bing_market=wt-WT&ex=-1 202 Ratelimit\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 330/34037 [1:26:02<115:28:36, 12.33s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 188 saved to /kaggle/working/batch_188.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 340/34037 [1:27:38<96:02:31, 10.26s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 189 saved to /kaggle/working/batch_189.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 350/34037 [1:29:43<142:31:27, 15.23s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 190 saved to /kaggle/working/batch_190.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 351/34037 [1:30:00<147:20:20, 15.75s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.curledup.com/chronsto.htm: HTTPSConnectionPool(host='www.curledup.com', port=443): Max retries exceeded with url: /chronsto.htm (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7a03f1abdea0>, 'Connection to www.curledup.com timed out. (connect timeout=10)'))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 356/34037 [1:32:27<183:09:13, 19.58s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://albas.al/botime/shpella-e-pirateve/: HTTPSConnectionPool(host='albas.al', port=443): Max retries exceeded with url: /botime/shpella-e-pirateve/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7a03f1aff880>: Failed to resolve 'albas.al' ([Errno -2] Name or service not known)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 360/34037 [1:34:37<218:42:55, 23.38s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 191 saved to /kaggle/working/batch_191.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 370/34037 [1:37:02<142:15:18, 15.21s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 192 saved to /kaggle/working/batch_192.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 375/34037 [1:38:41<192:42:31, 20.61s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://albanianhistory.org/albanianliterature/authors_classical/koliqi.html: HTTPSConnectionPool(host='albanianhistory.org', port=443): Max retries exceeded with url: /albanianliterature/authors_classical/koliqi.html (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 380/34037 [1:40:43<192:41:02, 20.61s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 193 saved to /kaggle/working/batch_193.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 381/34037 [1:41:06<199:36:25, 21.35s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://albanianhistory.org/albanianliterature/authors/classical/spasse/index.html: HTTPSConnectionPool(host='albanianhistory.org', port=443): Max retries exceeded with url: /albanianliterature/authors/classical/spasse/index.html (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 390/34037 [1:44:27<206:50:56, 22.13s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 194 saved to /kaggle/working/batch_194.csv\nError processing https://fjalashqip.com/naim-frasheri-fjalet-e-qiririt/: HTTPSConnectionPool(host='fjalashqip.com', port=443): Max retries exceeded with url: /naim-frasheri-fjalet-e-qiririt/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 400/34037 [1:48:03<179:30:39, 19.21s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 195 saved to /kaggle/working/batch_195.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 410/34037 [1:49:22<89:10:43,  9.55s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 196 saved to /kaggle/working/batch_196.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 420/34037 [1:50:52<78:28:03,  8.40s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 197 saved to /kaggle/working/batch_197.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 430/34037 [1:52:29<86:31:45,  9.27s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 198 saved to /kaggle/working/batch_198.csv\nError processing https://cuddlebuggery.com/blog/2011/11/18/review-liesl-and-po/: HTTPSConnectionPool(host='cuddlebuggery.com', port=443): Max retries exceeded with url: /blog/2011/11/18/review-liesl-and-po/ (Caused by ResponseError('too many 502 error responses'))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 440/34037 [1:55:22<110:38:11, 11.85s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 199 saved to /kaggle/working/batch_199.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 447/34037 [1:56:16<74:59:25,  8.04s/it] ","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.amystewart.com/books/drunkenbotanist/: HTTPSConnectionPool(host='www.amystewart.com', port=443): Max retries exceeded with url: /books/drunkenbotanist/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 448/34037 [1:56:50<144:55:36, 15.53s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://alcoholexplained.com/: HTTPSConnectionPool(host='alcoholexplained.com', port=443): Max retries exceeded with url: / (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 450/34037 [1:57:33<164:06:52, 17.59s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 200 saved to /kaggle/working/batch_200.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 460/34037 [1:59:18<71:53:07,  7.71s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 201 saved to /kaggle/working/batch_201.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 470/34037 [2:00:36<67:42:14,  7.26s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 202 saved to /kaggle/working/batch_202.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 480/34037 [2:02:10<121:54:47, 13.08s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 203 saved to /kaggle/working/batch_203.csv\nError processing https://varianjohnson.com/books/playing-the-cards-youre-dealt/: HTTPSConnectionPool(host='varianjohnson.com', port=443): Max retries exceeded with url: /books/playing-the-cards-youre-dealt/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 490/34037 [2:04:06<82:51:40,  8.89s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 204 saved to /kaggle/working/batch_204.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 492/34037 [2:04:49<145:56:50, 15.66s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.fullofbooks.com/flow-down-like-silver-hypatia-of-alexandria-by-ki-longfellow-review/: HTTPSConnectionPool(host='www.fullofbooks.com', port=443): Max retries exceeded with url: /flow-down-like-silver-hypatia-of-alexandria-by-ki-longfellow-review/ (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.fullofbooks.com', port=443): Read timed out. (read timeout=10)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 493/34037 [2:06:27<374:07:44, 40.15s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://reviews.metaphorosis.com/review/justine-lawrence-durrell/: HTTPSConnectionPool(host='reviews.metaphorosis.com', port=443): Max retries exceeded with url: /review/justine-lawrence-durrell/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 500/34037 [2:07:57<120:04:08, 12.89s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 205 saved to /kaggle/working/batch_205.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 509/34037 [2:10:15<74:49:50,  8.03s/it] ","output_type":"stream"},{"name":"stdout","text":"Error processing https://ib.beaconhouse.net/a/pdf/goto/CD/the_rediscovery_of_man_cordwainer_smith.pdf: HTTPSConnectionPool(host='ib.beaconhouse.net', port=443): Max retries exceeded with url: /a/pdf/goto/CD/the_rediscovery_of_man_cordwainer_smith.pdf (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7a03f1ac16c0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|▏         | 510/34037 [2:11:09<204:59:39, 22.01s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 206 saved to /kaggle/working/batch_206.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 520/34037 [2:12:36<79:37:14,  8.55s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 207 saved to /kaggle/working/batch_207.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 530/34037 [2:14:29<132:29:02, 14.23s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 208 saved to /kaggle/working/batch_208.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 540/34037 [2:16:25<81:13:02,  8.73s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 209 saved to /kaggle/working/batch_209.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 550/34037 [2:18:22<115:09:14, 12.38s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 210 saved to /kaggle/working/batch_210.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 560/34037 [2:21:07<227:53:22, 24.51s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 211 saved to /kaggle/working/batch_211.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 570/34037 [2:23:31<102:17:05, 11.00s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 212 saved to /kaggle/working/batch_212.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 580/34037 [2:24:55<99:10:45, 10.67s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 213 saved to /kaggle/working/batch_213.csv\nError processing https://www.logicmatters.net/2008/06/08/awodeys-category-theory-ch-1/: HTTPSConnectionPool(host='www.logicmatters.net', port=443): Max retries exceeded with url: /2008/06/08/awodeys-category-theory-ch-1/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 590/34037 [2:27:34<113:50:10, 12.25s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 214 saved to /kaggle/working/batch_214.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 597/34037 [2:28:57<104:03:27, 11.20s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.beeartless.com/blog/2019/book-reviews/the-fall-albert-camus/: HTTPSConnectionPool(host='www.beeartless.com', port=443): Max retries exceeded with url: /blog/2019/book-reviews/the-fall-albert-camus/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7a03e3ddaf80>: Failed to resolve 'www.beeartless.com' ([Errno -2] Name or service not known)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 600/34037 [2:30:58<263:26:29, 28.36s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 215 saved to /kaggle/working/batch_215.csv\nError processing https://www.kirkus.us-east-1.elasticbeanstalk.com/book-reviews/boualem-sansal/the-german-mujahid/: HTTPSConnectionPool(host='www.kirkus.us-east-1.elasticbeanstalk.com', port=443): Max retries exceeded with url: /book-reviews/boualem-sansal/the-german-mujahid/ (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'www.kirkus.us-east-1.elasticbeanstalk.com'. (_ssl.c:1007)\")))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 610/34037 [2:34:03<113:01:19, 12.17s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 216 saved to /kaggle/working/batch_216.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 613/34037 [2:34:53<129:27:52, 13.94s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.fullofbooks.com/the-eight-by-katherine-neville-review/: HTTPSConnectionPool(host='www.fullofbooks.com', port=443): Max retries exceeded with url: /the-eight-by-katherine-neville-review/ (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.fullofbooks.com', port=443): Read timed out. (read timeout=10)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 619/34037 [2:37:47<184:40:11, 19.89s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://coalhillreview.com/book-review-the-sheltering-sky-by-paul-bowles/: HTTPSConnectionPool(host='coalhillreview.com', port=443): Max retries exceeded with url: /book-review-the-sheltering-sky-by-paul-bowles/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 620/34037 [2:38:40<276:09:16, 29.75s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 217 saved to /kaggle/working/batch_217.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 630/34037 [2:41:00<117:43:00, 12.69s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 218 saved to /kaggle/working/batch_218.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 632/34037 [2:41:44<161:42:16, 17.43s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://blogs.lse.ac.uk/mec/2019/06/26/book-review-a-history-of-algeria-by-james-mcdougall/: HTTPSConnectionPool(host='blogs.lse.ac.uk', port=443): Max retries exceeded with url: /mec/2019/06/26/book-review-a-history-of-algeria-by-james-mcdougall/ (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='blogs.lse.ac.uk', port=443): Read timed out. (read timeout=10)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 635/34037 [2:44:02<291:00:29, 31.36s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://islamicliterarysociety.com/2020/08/desert-encounter/: HTTPSConnectionPool(host='islamicliterarysociety.com', port=443): Max retries exceeded with url: /2020/08/desert-encounter/ (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='islamicliterarysociety.com', port=443): Read timed out. (read timeout=10)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 640/34037 [2:46:48<215:50:20, 23.27s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 219 saved to /kaggle/working/batch_219.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 650/34037 [2:49:24<114:29:17, 12.34s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 220 saved to /kaggle/working/batch_220.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 660/34037 [2:51:22<90:02:07,  9.71s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 221 saved to /kaggle/working/batch_221.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 663/34037 [2:52:06<132:17:23, 14.27s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.flipkart.com/data-structures-algorithms-made-easy/p/itmezunqtqmhwwcf?pid=9788193245279&lid=LSTBOK9788193245279EOP6SE&marketplace=FLIPKART: HTTPSConnectionPool(host='www.flipkart.com', port=443): Max retries exceeded with url: /data-structures-algorithms-made-easy/p/itmezunqtqmhwwcf?pid=9788193245279&lid=LSTBOK9788193245279EOP6SE&marketplace=FLIPKART (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.flipkart.com', port=443): Read timed out. (read timeout=10)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 664/34037 [2:53:45<366:05:58, 39.49s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://prod2.galleries.thebarnyardstore.com/files/uploaded-files/Documents/introduction_to_algorithms_a_creative_approach_by_udi_manber.pdf: HTTPSConnectionPool(host='prod2.galleries.thebarnyardstore.com', port=443): Max retries exceeded with url: /files/uploaded-files/Documents/introduction_to_algorithms_a_creative_approach_by_udi_manber.pdf (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7a03f2a2ca90>, 'Connection to prod2.galleries.thebarnyardstore.com timed out. (connect timeout=10)'))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 670/34037 [2:56:57<220:47:19, 23.82s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 222 saved to /kaggle/working/batch_222.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 674/34037 [2:57:31<110:30:04, 11.92s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.semanticscholar.org/paper/Problem-solving-with-algorithms-and-data-structures-Miller-Ranum/23da04d438068639f39d8b7871881ad03a48c0a0: HTTPSConnectionPool(host='www.semanticscholar.org', port=443): Max retries exceeded with url: /paper/Problem-solving-with-algorithms-and-data-structures-Miller-Ranum/23da04d438068639f39d8b7871881ad03a48c0a0 (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.semanticscholar.org', port=443): Read timed out. (read timeout=10)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 680/34037 [3:00:43<179:43:54, 19.40s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 223 saved to /kaggle/working/batch_223.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 690/34037 [3:02:04<104:06:50, 11.24s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 224 saved to /kaggle/working/batch_224.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 700/34037 [3:04:09<89:30:51,  9.67s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 225 saved to /kaggle/working/batch_225.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 710/34037 [3:05:48<52:07:05,  5.63s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 226 saved to /kaggle/working/batch_226.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 720/34037 [3:07:44<77:53:17,  8.42s/it] ","output_type":"stream"},{"name":"stdout","text":"Batch 227 saved to /kaggle/working/batch_227.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 730/34037 [3:09:12<91:25:28,  9.88s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 228 saved to /kaggle/working/batch_228.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 731/34037 [3:10:15<238:10:20, 25.74s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.fullofbooks.com/the-last-hour-of-gann-by-r-lee-smith-review/: HTTPSConnectionPool(host='www.fullofbooks.com', port=443): Max retries exceeded with url: /the-last-hour-of-gann-by-r-lee-smith-review/ (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.fullofbooks.com', port=443): Read timed out. (read timeout=10)\"))\nError processing https://theghastlygrimoire.com/2019/01/31/book-review-the-last-hour-of-gann-by-r-lee-smith/: HTTPSConnectionPool(host='theghastlygrimoire.com', port=443): Max retries exceeded with url: /2019/01/31/book-review-the-last-hour-of-gann-by-r-lee-smith/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7a03f16b8f70>: Failed to resolve 'theghastlygrimoire.com' ([Errno -2] Name or service not known)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 736/34037 [3:13:05<184:40:21, 19.96s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://mademoisellesmut.com/2020/04/26/barbarians-heart-ice-planet-barbarians-9-by-ruby-dixon/: HTTPSConnectionPool(host='mademoisellesmut.com', port=443): Max retries exceeded with url: /2020/04/26/barbarians-heart-ice-planet-barbarians-9-by-ruby-dixon/ (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7a03e3d4b760>: Failed to resolve 'mademoisellesmut.com' ([Errno -2] Name or service not known)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 740/34037 [3:14:13<137:37:10, 14.88s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 229 saved to /kaggle/working/batch_229.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 750/34037 [3:16:46<206:06:43, 22.29s/it]","output_type":"stream"},{"name":"stdout","text":"Batch 230 saved to /kaggle/working/batch_230.csv\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 751/34037 [3:16:46<145:05:58, 15.69s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing Fatherland (Paperback) by Robert   Harris: https://links.duckduckgo.com/d.js?q=Fatherland+%28Paperback%29+Robert+++Harris+book+review&kl=wt-wt&l=wt-wt&p=&s=0&df=&vqd=4-91934871852219133148408823060287659694&bing_market=wt-WT&ex=-1 202 Ratelimit\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 755/34037 [3:17:34<134:47:12, 14.58s/it]","output_type":"stream"}]},{"cell_type":"code","source":"print(\"2707 batch 361\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"test2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}