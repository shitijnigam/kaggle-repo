{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9218865,"sourceType":"datasetVersion","datasetId":5562194}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-21T14:21:42.835102Z","iopub.execute_input":"2024-08-21T14:21:42.835750Z","iopub.status.idle":"2024-08-21T14:21:43.880663Z","shell.execute_reply.started":"2024-08-21T14:21:42.835718Z","shell.execute_reply":"2024-08-21T14:21:43.879330Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/book-review-samples/goodreads_fantasy.csv\n/kaggle/input/book-review-samples/goodreads_all_genres.csv\n/kaggle/input/book-review-samples/Darth_Plagueis_reviews(4).csv\n/kaggle/input/book-review-samples/Dune_Book_1_reviews.csv\n/kaggle/input/book-review-samples/Dune_Book_1_reviews(4).csv\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install duckduckgo-search","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:21:43.882874Z","iopub.execute_input":"2024-08-21T14:21:43.883453Z","iopub.status.idle":"2024-08-21T14:21:57.807539Z","shell.execute_reply.started":"2024-08-21T14:21:43.883415Z","shell.execute_reply":"2024-08-21T14:21:57.806309Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting duckduckgo-search\n  Downloading duckduckgo_search-6.2.10-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: click>=8.1.7 in /opt/conda/lib/python3.10/site-packages (from duckduckgo-search) (8.1.7)\nCollecting primp>=0.6.1 (from duckduckgo-search)\n  Downloading primp-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nDownloading duckduckgo_search-6.2.10-py3-none-any.whl (27 kB)\nDownloading primp-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: primp, duckduckgo-search\nSuccessfully installed duckduckgo-search-6.2.10 primp-0.6.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n# import pandas as pd\nfrom datetime import datetime\nimport time\nimport random\nimport re\nimport html\nfrom duckduckgo_search import DDGS","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:21:57.809304Z","iopub.execute_input":"2024-08-21T14:21:57.809741Z","iopub.status.idle":"2024-08-21T14:21:58.076485Z","shell.execute_reply.started":"2024-08-21T14:21:57.809699Z","shell.execute_reply":"2024-08-21T14:21:58.075582Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# boilerplate removal\n\ndef remove_boilerplate(text):\n    boilerplate = ['cookie policy', 'privacy policy', 'terms of service', 'all rights reserved', '\\n']\n    for phrase in boilerplate:\n        text = re.sub(r'(?i)' + re.escape(phrase) + r'.*', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:21:58.078350Z","iopub.execute_input":"2024-08-21T14:21:58.078876Z","iopub.status.idle":"2024-08-21T14:21:58.084042Z","shell.execute_reply.started":"2024-08-21T14:21:58.078848Z","shell.execute_reply":"2024-08-21T14:21:58.083054Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# comment truncation (20% threshold default)\n\ndef truncate_at_comments(review_text, threshold_percentage=20, min_review_length=100):\n    comment_identifiers = [\n        'Comments', 'Leave a comment', 'Reader comments', \n        'What do you think?', 'Join the discussion', 'Add a comment',\n        'Post a comment', 'Write a comment', 'Show comments'\n    ]\n    \n    lower_text = review_text.lower()\n    text_length = len(lower_text)\n    threshold = max(int(text_length * (threshold_percentage / 100)), min_review_length)\n\n    # Check for comment identifiers\n    for identifier in comment_identifiers:\n        index = lower_text.find(identifier.lower())\n        if index != -1 and index > threshold:\n            return review_text[:index].strip()\n    \n    # If no identifiers found, try to detect comment-like structures\n    paragraphs = review_text.split('\\n\\n')\n    filtered_paragraphs = []\n    \n    for paragraph in paragraphs:\n        # Skip short paragraphs that might be comments\n        if len(paragraph) < 50:\n            continue\n        \n        # Skip paragraphs that start with common comment patterns\n        if re.match(r'^(Posted by|From|User|Anonymous|[\\d/]+:)', paragraph.strip()):\n            continue\n        \n        filtered_paragraphs.append(paragraph)\n    \n    # If we've removed some paragraphs, join the remaining ones\n    if len(filtered_paragraphs) < len(paragraphs):\n        return '\\n\\n'.join(filtered_paragraphs).strip()\n    \n    # If we haven't removed any paragraphs, return at least the first part of the text\n    return review_text[:max(threshold, len(review_text) // 2)].strip()","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:21:58.085478Z","iopub.execute_input":"2024-08-21T14:21:58.085776Z","iopub.status.idle":"2024-08-21T14:21:58.098207Z","shell.execute_reply.started":"2024-08-21T14:21:58.085751Z","shell.execute_reply":"2024-08-21T14:21:58.097023Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# remove comment classes\n\ndef remove_comments(review_soup):\n    # Common class names for comment sections\n    comment_classes = ['comment', 'comments-list', 'comments-area', 'comments', 'comment-section', 'user-comments', 'disqus_thread']\n    \n    for class_name in comment_classes:\n        comment_section = review_soup.find('div', class_=class_name)\n        if comment_section:\n            comment_section.decompose()  # This removes the element from the soup\n    \n    return review_soup","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:21:58.099244Z","iopub.execute_input":"2024-08-21T14:21:58.099590Z","iopub.status.idle":"2024-08-21T14:21:58.113448Z","shell.execute_reply.started":"2024-08-21T14:21:58.099564Z","shell.execute_reply":"2024-08-21T14:21:58.112431Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# paragraph scoring for review content\n\ndef score_paragraph(paragraph):\n    review_keywords = ['review', 'book', 'read', 'author', 'story', 'character', 'plot', 'recommend']\n    return sum(keyword in paragraph.lower() for keyword in review_keywords)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:22:07.556419Z","iopub.execute_input":"2024-08-21T14:22:07.557510Z","iopub.status.idle":"2024-08-21T14:22:07.562985Z","shell.execute_reply.started":"2024-08-21T14:22:07.557471Z","shell.execute_reply":"2024-08-21T14:22:07.561616Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# add retries with timeouts selectively\n\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\ndef create_session_with_retries():\n    session = requests.Session()\n    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retries)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n    return session","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:22:08.719589Z","iopub.execute_input":"2024-08-21T14:22:08.719990Z","iopub.status.idle":"2024-08-21T14:22:08.726268Z","shell.execute_reply.started":"2024-08-21T14:22:08.719959Z","shell.execute_reply":"2024-08-21T14:22:08.725085Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"exceptions = ('google.', 'goodreads.com', 'amazon.', 'reddit.com', 'thestorygraph', 'youtube','.tv', 'barnesandnoble.com','wikipedia.','quora.com','sparknotes.com', 'grimdarkmagazine.')\n\ndef process_review_url(review_url, headers):\n    reviews = []\n    # Skip known non-review sites\n    # if any(site in review_url for site in ['google.com', 'wikipedia.org']):\n    if any(site in review_url.lower() for site in exceptions):\n        return reviews\n    \n    session = create_session_with_retries()\n    \n    try:\n        # Fetch the review page\n        # review_response = requests.get(review_url, headers=headers, timeout=10)\n        review_response = session.get(review_url, headers=headers, timeout=20)  # Increased timeout to 20 seconds\n        review_soup = BeautifulSoup(review_response.text, 'html.parser')\n        \n        # Remove Comments\n        review_soup = remove_comments(review_soup)\n        \n        # Review elements\n        review_elements = review_soup.find_all(['main','p'])\n        scored_paragraphs = [(elem, score_paragraph(elem.text)) for elem in review_elements]\n        \n        # Extract review text\n        review_paragraphs = [elem.text for elem, score in sorted(scored_paragraphs, key=lambda x: x[1], reverse=True)[:3]]\n        \n        # Search for relevant text\n        review_text = ' '.join(review_paragraphs)\n        \n        # Remove boilerplate\n        review_text = review_text.replace('\\n', ' ')\n        review_text = remove_boilerplate(review_text)\n        \n        # Truncate at Comments\n        review_text = truncate_at_comments(review_text)\n        \n        review_date = 'Unknown'\n        \n        reviews.append({\n            'review_text': review_text[:5000],  # Limit to first 5000 characters\n            'review_date': review_date,\n            'review_website': review_url\n        })\n        \n    except Exception as e:\n        print(f\"Error processing {review_url}: {str(e)}\")\n    \n    # Be polite to servers\n    time.sleep(random.uniform(1, 3))\n    \n    return reviews","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:38:25.577519Z","iopub.execute_input":"2024-08-21T14:38:25.577932Z","iopub.status.idle":"2024-08-21T14:38:25.589687Z","shell.execute_reply.started":"2024-08-21T14:38:25.577903Z","shell.execute_reply":"2024-08-21T14:38:25.588529Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# search_book_review test\n\ndef search_book_reviews(book_name, author):\n    # Combine book name and author for search query\n    search_query = f\"{book_name} {author} book review\"\n    \n    # print(f\"Searching for: {search_query}\")\n    \n    reviews = []\n    \n    # Define headers\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n    \n    # Use DDGS for searching\n    with DDGS() as ddgs:\n        results = ddgs.text(search_query, max_results=10)  # Adjust max_results as needed\n        \n        for result in results:\n            review_url = result['href']\n            reviews.extend(process_review_url(review_url, headers))\n    \n    # Create DataFrame\n    df = pd.DataFrame(reviews)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:38:29.555060Z","iopub.execute_input":"2024-08-21T14:38:29.555900Z","iopub.status.idle":"2024-08-21T14:38:29.562467Z","shell.execute_reply.started":"2024-08-21T14:38:29.555861Z","shell.execute_reply":"2024-08-21T14:38:29.561213Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Example usage\nbook_name = \"Darth Plagueis\"\nauthor = \"James Luceno\"\n\ndf = search_book_reviews(book_name, author)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:38:30.420987Z","iopub.execute_input":"2024-08-21T14:38:30.421403Z","iopub.status.idle":"2024-08-21T14:38:50.330025Z","shell.execute_reply.started":"2024-08-21T14:38:30.421369Z","shell.execute_reply":"2024-08-21T14:38:50.328972Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Save to CSV\ndf.to_csv(f\"{book_name.replace(' ', '_')}_reviews.csv\", index=False)\nprint(f\"Reviews saved to {book_name.replace(' ', '_')}_reviews.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:38:50.331652Z","iopub.execute_input":"2024-08-21T14:38:50.332004Z","iopub.status.idle":"2024-08-21T14:38:50.339594Z","shell.execute_reply.started":"2024-08-21T14:38:50.331973Z","shell.execute_reply":"2024-08-21T14:38:50.338665Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Reviews saved to Darth_Plagueis_reviews.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:38:50.340940Z","iopub.execute_input":"2024-08-21T14:38:50.341348Z","iopub.status.idle":"2024-08-21T14:38:50.356744Z","shell.execute_reply.started":"2024-08-21T14:38:50.341313Z","shell.execute_reply":"2024-08-21T14:38:50.355797Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                         review_text review_date  \\\n0  Pre-publication book reviews and features keep...     Unknown   \n1  Read more of our  book reviews  .  But perhaps...     Unknown   \n2  James Luceno’s 2012 novel Darth Plagueis, one ...     Unknown   \n3  Star Wars: Darth PlagueisStar Wars: Darth Plag...     Unknown   \n4  Add to Bookshelf    Read An Excerpt  Buy    Lo...     Unknown   \n\n                                      review_website  \n0  https://www.kirkusreviews.com/book-reviews/jam...  \n1  https://www.gamesradar.com/star-wars-darth-pla...  \n2  https://greatbooksguy.com/2023/06/17/book-revi...  \n3  https://www.ign.com/articles/2012/02/03/star-w...  \n4  https://www.penguinrandomhouse.com/books/7859/...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_text</th>\n      <th>review_date</th>\n      <th>review_website</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pre-publication book reviews and features keep...</td>\n      <td>Unknown</td>\n      <td>https://www.kirkusreviews.com/book-reviews/jam...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Read more of our  book reviews  .  But perhaps...</td>\n      <td>Unknown</td>\n      <td>https://www.gamesradar.com/star-wars-darth-pla...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>James Luceno’s 2012 novel Darth Plagueis, one ...</td>\n      <td>Unknown</td>\n      <td>https://greatbooksguy.com/2023/06/17/book-revi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Star Wars: Darth PlagueisStar Wars: Darth Plag...</td>\n      <td>Unknown</td>\n      <td>https://www.ign.com/articles/2012/02/03/star-w...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Add to Bookshelf    Read An Excerpt  Buy    Lo...</td>\n      <td>Unknown</td>\n      <td>https://www.penguinrandomhouse.com/books/7859/...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import csv\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:38:50.359257Z","iopub.execute_input":"2024-08-21T14:38:50.359974Z","iopub.status.idle":"2024-08-21T14:38:50.366619Z","shell.execute_reply.started":"2024-08-21T14:38:50.359935Z","shell.execute_reply":"2024-08-21T14:38:50.365641Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"input_csv = '/kaggle/input/book-review-samples/goodreads_all_genres.csv'\noutput_csv = 'book_reviews.csv'\n\ndf = pd.read_csv(input_csv)\ndf_unique = df.drop_duplicates(subset=['Title', 'Authors'], keep='first')\nnum_duplicates = len(df) - len(df_unique)\nnum_total = len(df)\nprint(f\"Removed {num_duplicates} duplicate entries out of {num_total}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:38:50.367794Z","iopub.execute_input":"2024-08-21T14:38:50.368091Z","iopub.status.idle":"2024-08-21T14:38:50.679185Z","shell.execute_reply.started":"2024-08-21T14:38:50.368067Z","shell.execute_reply":"2024-08-21T14:38:50.678200Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Removed 115123 duplicate entries out of 118700\n","output_type":"stream"}]},{"cell_type":"code","source":"def process_book_list(input_csv, output_csv):\n    # Read the input CSV\n    df = pd.read_csv(input_csv)\n    \n    # remove duplicates\n    df_unique = df.drop_duplicates(subset=['Title', 'Authors'], keep='first')\n    \n    # Print information about removed duplicates\n    num_duplicates = len(df) - len(df_unique)\n    num_total = len(df)\n    print(f\"Removed {num_duplicates} duplicate entries out of {num_total}\")\n    \n    all_reviews = []\n    \n    # Iterate through each row in the dataframe\n    for index, row in tqdm(df_unique.iterrows(), total=df_unique.shape[0], desc=\"Processing books\"):\n        title = row['Title']\n        authors = row['Authors']\n        \n        try:\n            # Get reviews for this book\n            reviews_df = search_book_reviews(title, authors)\n            \n            # Add book information to each review\n            reviews_df['Title'] = title\n            reviews_df['Authors'] = authors\n            reviews_df['Avg Ratings'] = row['Avg Ratings']\n            reviews_df['Rating'] = row['Rating']\n            reviews_df['Published_year'] = row['Published_year']\n            \n            all_reviews.append(reviews_df)\n        \n        except Exception as e:\n            print(f\"Error processing {title} by {authors}: {str(e)}\")\n    \n    # Combine all reviews into a single dataframe\n    if all_reviews:\n        final_df = pd.concat(all_reviews, ignore_index=True)\n        \n        # Save to CSV\n        final_df.to_csv(output_csv, index=False, quoting=csv.QUOTE_ALL)\n        print(f\"Reviews saved to {output_csv}\")\n    else:\n        print(\"No reviews were collected.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:41:53.514264Z","iopub.execute_input":"2024-08-21T14:41:53.515260Z","iopub.status.idle":"2024-08-21T14:41:53.524511Z","shell.execute_reply.started":"2024-08-21T14:41:53.515220Z","shell.execute_reply":"2024-08-21T14:41:53.523328Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Usage\nprocess_book_list(input_csv, output_csv)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T14:41:55.245340Z","iopub.execute_input":"2024-08-21T14:41:55.245937Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Removed 115123 duplicate entries out of 118700\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   1%|          | 36/3577 [05:33<11:27:53, 11.66s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://gavthorpe.co.uk/books/deliverance-lost-by-gav-thorpe/: HTTPSConnectionPool(host='gavthorpe.co.uk', port=443): Max retries exceeded with url: /books/deliverance-lost-by-gav-thorpe/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 54/3577 [09:36<21:50:08, 22.31s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.suzannecollinsbooks.com/the_hunger_games_69765.htm: HTTPSConnectionPool(host='www.suzannecollinsbooks.com', port=443): Max retries exceeded with url: /the_hunger_games_69765.htm (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 59/3577 [10:59<18:19:58, 18.76s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.washingtonpost.com/entertainment/books/book-review-the-goldfinch-by-donna-tartt/2013/10/22/39b556ac-3837-11e3-ae46-e4248e75c8ea_story.html: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Max retries exceeded with url: /entertainment/books/book-review-the-goldfinch-by-donna-tartt/2013/10/22/39b556ac-3837-11e3-ae46-e4248e75c8ea_story.html (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=20)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   2%|▏         | 64/3577 [14:02<21:35:03, 22.12s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.suzannecollinsbooks.com/mockingjay_102797.htm: HTTPSConnectionPool(host='www.suzannecollinsbooks.com', port=443): Max retries exceeded with url: /mockingjay_102797.htm (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   3%|▎         | 105/3577 [26:47<11:41:43, 12.13s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.thatartsyreadergirl.com/2020/05/beach-read-by-emily-henry-book-review/: HTTPSConnectionPool(host='www.thatartsyreadergirl.com', port=443): Max retries exceeded with url: /2020/05/beach-read-by-emily-henry-book-review/ (Caused by ResponseError('too many 500 error responses'))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   3%|▎         | 109/3577 [29:14<22:48:40, 23.68s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://yourbookcorner.com/the-love-hypothesis-summary-review/: HTTPSConnectionPool(host='yourbookcorner.com', port=443): Max retries exceeded with url: /the-love-hypothesis-summary-review/ (Caused by ResponseError('too many 500 error responses'))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   3%|▎         | 120/3577 [35:19<31:14:49, 32.54s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.washingtonpost.com/entertainment/books/people-we-meet-on-vacation/2021/05/11/0773422a-b274-11eb-ab43-bebddc5a0f65_story.html: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Max retries exceeded with url: /entertainment/books/people-we-meet-on-vacation/2021/05/11/0773422a-b274-11eb-ab43-bebddc5a0f65_story.html (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=20)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   4%|▍         | 137/3577 [42:09<13:53:05, 14.53s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://stieglarsson.com/The-Girl-With-The-Dragon-Tattoo/: HTTPSConnectionPool(host='stieglarsson.com', port=443): Max retries exceeded with url: /The-Girl-With-The-Dragon-Tattoo/ (Caused by ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   4%|▍         | 138/3577 [42:25<14:15:06, 14.92s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://yourbookcorner.com/verity-book-summary-review/: HTTPSConnectionPool(host='yourbookcorner.com', port=443): Max retries exceeded with url: /verity-book-summary-review/ (Caused by ResponseError('too many 500 error responses'))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   4%|▍         | 140/3577 [43:39<25:01:44, 26.22s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.bloomsbury.com/us/house-of-earth-and-blood-9781635574043/: HTTPSConnectionPool(host='www.bloomsbury.com', port=443): Max retries exceeded with url: /us/house-of-earth-and-blood-9781635574043/ (Caused by ResponseError('too many 500 error responses'))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   4%|▍         | 141/3577 [44:08<25:42:22, 26.93s/it]","output_type":"stream"},{"name":"stdout","text":"Error processing https://www.washingtonpost.com/entertainment/books/fifty-shades-has-come-to-an-end-what-have-we-learned-from-this-provocative-romance/2021/06/28/942213e2-d680-11eb-9f29-e9e6c9e843c6_story.html: HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Max retries exceeded with url: /entertainment/books/fifty-shades-has-come-to-an-end-what-have-we-learned-from-this-provocative-romance/2021/06/28/942213e2-d680-11eb-9f29-e9e6c9e843c6_story.html (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.washingtonpost.com', port=443): Read timed out. (read timeout=20)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   4%|▍         | 149/3577 [48:07<26:50:28, 28.19s/it]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}