{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9208293,"sourceType":"datasetVersion","datasetId":5562194}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T13:52:22.686547Z","iopub.execute_input":"2024-08-20T13:52:22.687071Z","iopub.status.idle":"2024-08-20T13:52:24.006366Z","shell.execute_reply.started":"2024-08-20T13:52:22.687030Z","shell.execute_reply":"2024-08-20T13:52:24.005016Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install duckduckgo-search","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:52:24.008644Z","iopub.execute_input":"2024-08-20T13:52:24.009270Z","iopub.status.idle":"2024-08-20T13:52:41.640499Z","shell.execute_reply.started":"2024-08-20T13:52:24.009236Z","shell.execute_reply":"2024-08-20T13:52:41.639151Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting duckduckgo-search\n  Downloading duckduckgo_search-6.2.9-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: click>=8.1.7 in /opt/conda/lib/python3.10/site-packages (from duckduckgo-search) (8.1.7)\nCollecting primp>=0.6.0 (from duckduckgo-search)\n  Downloading primp-0.6.0-cp38-abi3-manylinux_2_28_x86_64.whl.metadata (11 kB)\nDownloading duckduckgo_search-6.2.9-py3-none-any.whl (27 kB)\nDownloading primp-0.6.0-cp38-abi3-manylinux_2_28_x86_64.whl (2.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: primp, duckduckgo-search\nSuccessfully installed duckduckgo-search-6.2.9 primp-0.6.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n# import pandas as pd\nfrom datetime import datetime\nimport time\nimport random\nimport re\nimport html\nfrom duckduckgo_search import DDGS","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:52:47.918093Z","iopub.execute_input":"2024-08-20T13:52:47.918543Z","iopub.status.idle":"2024-08-20T13:52:48.283567Z","shell.execute_reply.started":"2024-08-20T13:52:47.918506Z","shell.execute_reply":"2024-08-20T13:52:48.282328Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# boilerplate removal\n\ndef remove_boilerplate(text):\n    boilerplate = ['cookie policy', 'privacy policy', 'terms of service', 'all rights reserved', '\\n']\n    for phrase in boilerplate:\n        text = re.sub(r'(?i)' + re.escape(phrase) + r'.*', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:52:57.279418Z","iopub.execute_input":"2024-08-20T13:52:57.280736Z","iopub.status.idle":"2024-08-20T13:52:57.286512Z","shell.execute_reply.started":"2024-08-20T13:52:57.280694Z","shell.execute_reply":"2024-08-20T13:52:57.285333Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# comment truncation (20% threshold default)\n\ndef truncate_at_comments(review_text, threshold_percentage=20, min_review_length=100):\n    comment_identifiers = [\n        'Comments', 'Leave a comment', 'Reader comments', \n        'What do you think?', 'Join the discussion', 'Add a comment',\n        'Post a comment', 'Write a comment', 'Show comments'\n    ]\n    \n    lower_text = review_text.lower()\n    text_length = len(lower_text)\n    threshold = max(int(text_length * (threshold_percentage / 100)), min_review_length)\n\n    # Check for comment identifiers\n    for identifier in comment_identifiers:\n        index = lower_text.find(identifier.lower())\n        if index != -1 and index > threshold:\n            return review_text[:index].strip()\n    \n    # If no identifiers found, try to detect comment-like structures\n    paragraphs = review_text.split('\\n\\n')\n    filtered_paragraphs = []\n    \n    for paragraph in paragraphs:\n        # Skip short paragraphs that might be comments\n        if len(paragraph) < 50:\n            continue\n        \n        # Skip paragraphs that start with common comment patterns\n        if re.match(r'^(Posted by|From|User|Anonymous|[\\d/]+:)', paragraph.strip()):\n            continue\n        \n        filtered_paragraphs.append(paragraph)\n    \n    # If we've removed some paragraphs, join the remaining ones\n    if len(filtered_paragraphs) < len(paragraphs):\n        return '\\n\\n'.join(filtered_paragraphs).strip()\n    \n    # If we haven't removed any paragraphs, return at least the first part of the text\n    return review_text[:max(threshold, len(review_text) // 2)].strip()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:52:59.598190Z","iopub.execute_input":"2024-08-20T13:52:59.598597Z","iopub.status.idle":"2024-08-20T13:52:59.609562Z","shell.execute_reply.started":"2024-08-20T13:52:59.598566Z","shell.execute_reply":"2024-08-20T13:52:59.608330Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# remove comment classes\n\ndef remove_comments(review_soup):\n    # Common class names for comment sections\n    comment_classes = ['comment', 'comments-list', 'comments-area', 'comments', 'comment-section', 'user-comments', 'disqus_thread']\n    \n    for class_name in comment_classes:\n        comment_section = review_soup.find('div', class_=class_name)\n        if comment_section:\n            comment_section.decompose()  # This removes the element from the soup\n    \n    return review_soup","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:53:01.228054Z","iopub.execute_input":"2024-08-20T13:53:01.228441Z","iopub.status.idle":"2024-08-20T13:53:01.235536Z","shell.execute_reply.started":"2024-08-20T13:53:01.228412Z","shell.execute_reply":"2024-08-20T13:53:01.234323Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# paragraph scoring for review content\n\ndef score_paragraph(paragraph):\n    review_keywords = ['review', 'book', 'read', 'author', 'story', 'character', 'plot', 'recommend']\n    return sum(keyword in paragraph.lower() for keyword in review_keywords)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T13:53:01.778028Z","iopub.execute_input":"2024-08-20T13:53:01.778408Z","iopub.status.idle":"2024-08-20T13:53:01.784851Z","shell.execute_reply.started":"2024-08-20T13:53:01.778380Z","shell.execute_reply":"2024-08-20T13:53:01.783519Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# add retries with timeouts selectively\n\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\n\ndef create_session_with_retries():\n    session = requests.Session()\n    retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n    adapter = HTTPAdapter(max_retries=retries)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n    return session","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:10:56.829606Z","iopub.execute_input":"2024-08-20T14:10:56.830822Z","iopub.status.idle":"2024-08-20T14:10:56.837689Z","shell.execute_reply.started":"2024-08-20T14:10:56.830773Z","shell.execute_reply":"2024-08-20T14:10:56.836452Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"exceptions = (\n    'google.com', \n    'goodreads.com', \n    'amazon.*', \n    'reddit.com', \n    'thestorygraph', \n    'youtube',\n    '*.tv', \n    'barnesandnoble.com',\n    'wikipedia.*',\n    'quora.com',\n    'sparknotes.com',\n)\n\ndef process_review_url(review_url, headers):\n    reviews = []\n    # Skip known non-review sites\n    # if any(site in review_url for site in ['google.com', 'wikipedia.org']):\n    if any(site in review_url for site in exceptions):\n        return reviews\n    \n    session = create_session_with_retries()\n    \n    try:\n        # Fetch the review page\n        # review_response = requests.get(review_url, headers=headers, timeout=10)\n        review_response = session.get(review_url, headers=headers, timeout=20)  # Increased timeout to 20 seconds\n        review_soup = BeautifulSoup(review_response.text, 'html.parser')\n        \n        # Remove Comments\n        review_soup = remove_comments(review_soup)\n        \n        # Review elements\n        review_elements = review_soup.find_all(['main','p'])\n        scored_paragraphs = [(elem, score_paragraph(elem.text)) for elem in review_elements]\n        \n        # Extract review text\n        review_paragraphs = [elem.text for elem, score in sorted(scored_paragraphs, key=lambda x: x[1], reverse=True)[:3]]\n        \n        # Search for relevant text\n        review_text = ' '.join(review_paragraphs)\n        \n        # Remove boilerplate\n        review_text = review_text.replace('\\n', ' ')\n        review_text = remove_boilerplate(review_text)\n        \n        # Truncate at Comments\n        review_text = truncate_at_comments(review_text)\n        \n        review_date = 'Unknown'\n        \n        reviews.append({\n            'review_text': review_text[:5000],  # Limit to first 5000 characters\n            'review_date': review_date,\n            'review_website': review_url\n        })\n        \n    except Exception as e:\n        print(f\"Error processing {review_url}: {str(e)}\")\n    \n    # Be polite to servers\n    time.sleep(random.uniform(1, 3))\n    \n    return reviews","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:10:57.079100Z","iopub.execute_input":"2024-08-20T14:10:57.079478Z","iopub.status.idle":"2024-08-20T14:10:57.092683Z","shell.execute_reply.started":"2024-08-20T14:10:57.079449Z","shell.execute_reply":"2024-08-20T14:10:57.091357Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# search_book_review test\n\ndef search_book_reviews(book_name, author):\n    # Combine book name and author for search query\n    search_query = f\"{book_name} {author} book review\"\n    \n    print(f\"Searching for: {search_query}\")\n    \n    reviews = []\n    \n    # Define headers\n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n    \n    # Use DDGS for searching\n    with DDGS() as ddgs:\n        results = ddgs.text(search_query, max_results=5)  # Adjust max_results as needed\n        \n        for result in results:\n            review_url = result['href']\n            reviews.extend(process_review_url(review_url, headers))\n    \n    # Create DataFrame\n    df = pd.DataFrame(reviews)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:18:41.326783Z","iopub.execute_input":"2024-08-20T14:18:41.327222Z","iopub.status.idle":"2024-08-20T14:18:41.335427Z","shell.execute_reply.started":"2024-08-20T14:18:41.327192Z","shell.execute_reply":"2024-08-20T14:18:41.334152Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Example usage\nbook_name = \"Darth Plagueis\"\nauthor = \"James Luceno\"\n\ndf = search_book_reviews(book_name, author)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:18:41.824588Z","iopub.execute_input":"2024-08-20T14:18:41.825035Z","iopub.status.idle":"2024-08-20T14:18:53.540871Z","shell.execute_reply.started":"2024-08-20T14:18:41.825000Z","shell.execute_reply":"2024-08-20T14:18:53.539695Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Searching for: Darth Plagueis James Luceno book review\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save to CSV\ndf.to_csv(f\"{book_name.replace(' ', '_')}_reviews.csv\", index=False)\nprint(f\"Reviews saved to {book_name.replace(' ', '_')}_reviews.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:18:53.543325Z","iopub.execute_input":"2024-08-20T14:18:53.543794Z","iopub.status.idle":"2024-08-20T14:18:53.553119Z","shell.execute_reply.started":"2024-08-20T14:18:53.543755Z","shell.execute_reply":"2024-08-20T14:18:53.551739Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Reviews saved to Darth_Plagueis_reviews.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:18:53.554671Z","iopub.execute_input":"2024-08-20T14:18:53.555114Z","iopub.status.idle":"2024-08-20T14:18:53.570393Z","shell.execute_reply.started":"2024-08-20T14:18:53.555083Z","shell.execute_reply":"2024-08-20T14:18:53.569170Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"                                         review_text review_date  \\\n0  Pre-publication book reviews and features keep...     Unknown   \n1  August 1, 2024 Star Wars: Darth Plagueis By Ja...     Unknown   \n2  Read more of our  book reviews  .  But perhaps...     Unknown   \n3  Star Wars: Darth PlagueisStar Wars: Darth Plag...     Unknown   \n\n                                      review_website  \n0  https://www.kirkusreviews.com/book-reviews/jam...  \n1  https://themptybookshelf.wordpress.com/2024/08...  \n2  https://www.gamesradar.com/star-wars-darth-pla...  \n3  https://www.ign.com/articles/2012/02/03/star-w...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_text</th>\n      <th>review_date</th>\n      <th>review_website</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Pre-publication book reviews and features keep...</td>\n      <td>Unknown</td>\n      <td>https://www.kirkusreviews.com/book-reviews/jam...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>August 1, 2024 Star Wars: Darth Plagueis By Ja...</td>\n      <td>Unknown</td>\n      <td>https://themptybookshelf.wordpress.com/2024/08...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Read more of our  book reviews  .  But perhaps...</td>\n      <td>Unknown</td>\n      <td>https://www.gamesradar.com/star-wars-darth-pla...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Star Wars: Darth PlagueisStar Wars: Darth Plag...</td>\n      <td>Unknown</td>\n      <td>https://www.ign.com/articles/2012/02/03/star-w...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import csv\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:18:53.573050Z","iopub.execute_input":"2024-08-20T14:18:53.573504Z","iopub.status.idle":"2024-08-20T14:18:53.582363Z","shell.execute_reply.started":"2024-08-20T14:18:53.573462Z","shell.execute_reply":"2024-08-20T14:18:53.581005Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def process_book_list(input_csv, output_csv):\n    # Read the input CSV\n    df = pd.read_csv(input_csv)\n    \n    all_reviews = []\n    \n    # Iterate through each row in the dataframe\n    for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing books\"):\n        title = row['Title']\n        authors = row['Authors']\n        \n        try:\n            # Get reviews for this book\n            reviews_df = search_book_reviews_api(title, authors)\n            \n            # Add book information to each review\n            reviews_df['Title'] = title\n            reviews_df['Authors'] = authors\n            reviews_df['Avg Ratings'] = row['Avg Ratings']\n            reviews_df['Rating'] = row['Rating']\n            reviews_df['Published_year'] = row['Published_year']\n            \n            all_reviews.append(reviews_df)\n        \n        except Exception as e:\n            print(f\"Error processing {title} by {authors}: {str(e)}\")\n    \n    # Combine all reviews into a single dataframe\n    if all_reviews:\n        final_df = pd.concat(all_reviews, ignore_index=True)\n        \n        # Save to CSV\n        final_df.to_csv(output_csv, index=False, quoting=csv.QUOTE_ALL)\n        print(f\"Reviews saved to {output_csv}\")\n    else:\n        print(\"No reviews were collected.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:18:53.583653Z","iopub.execute_input":"2024-08-20T14:18:53.584055Z","iopub.status.idle":"2024-08-20T14:18:53.595641Z","shell.execute_reply.started":"2024-08-20T14:18:53.584010Z","shell.execute_reply":"2024-08-20T14:18:53.594497Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Usage\ninput_csv = '/kaggle/input/book-review-samples/goodreads_fantasy.csv'\noutput_csv = 'book_reviews.csv'\nprocess_book_list(input_csv, output_csv)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T14:18:53.597131Z","iopub.execute_input":"2024-08-20T14:18:53.597498Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Processing books:   0%|          | 0/1250 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Searching for: Harry Potter and the Sorcerer’s Stone (Harry Potter, #1) J.K. Rowling book review\nError processing https://www.sparknotes.com/lit/harrypotter/summary/: HTTPSConnectionPool(host='www.sparknotes.com', port=443): Max retries exceeded with url: /lit/harrypotter/summary/ (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.sparknotes.com', port=443): Read timed out. (read timeout=20)\"))\nError processing https://www.sparknotes.com/lit/harrypotter/: HTTPSConnectionPool(host='www.sparknotes.com', port=443): Max retries exceeded with url: /lit/harrypotter/ (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.sparknotes.com', port=443): Read timed out. (read timeout=20)\"))\n","output_type":"stream"},{"name":"stderr","text":"Processing books:   0%|          | 1/1250 [03:25<71:13:57, 205.31s/it]","output_type":"stream"},{"name":"stdout","text":"Searching for: Harry Potter and the Chamber of Secrets (Harry Potter, #2) J.K. Rowling book review\nError processing https://www.sparknotes.com/lit/potter2/summary/: HTTPSConnectionPool(host='www.sparknotes.com', port=443): Max retries exceeded with url: /lit/potter2/summary/ (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.sparknotes.com', port=443): Read timed out. (read timeout=20)\"))\nError processing https://www.sparknotes.com/lit/potter2/: HTTPSConnectionPool(host='www.sparknotes.com', port=443): Max retries exceeded with url: /lit/potter2/ (Caused by ReadTimeoutError(\"HTTPSConnectionPool(host='www.sparknotes.com', port=443): Read timed out. (read timeout=20)\"))\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}