{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nigamshitij/lego-instructions-using-transformers?scriptVersionId=194559915\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-29T12:32:29.95142Z","iopub.execute_input":"2024-08-29T12:32:29.952044Z","iopub.status.idle":"2024-08-29T12:32:29.959932Z","shell.execute_reply.started":"2024-08-29T12:32:29.951991Z","shell.execute_reply":"2024-08-29T12:32:29.958479Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-29T12:32:30.353327Z","iopub.execute_input":"2024-08-29T12:32:30.354614Z","iopub.status.idle":"2024-08-29T12:32:30.362503Z","shell.execute_reply.started":"2024-08-29T12:32:30.354562Z","shell.execute_reply":"2024-08-29T12:32:30.360971Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class LEGOInstructionDataset(Dataset):\n    def __init__(self, instruction_paths, prompts, transform=None):\n        self.instruction_paths = instruction_paths\n        self.prompts = prompts\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.instruction_paths)\n\n    def __getitem__(self, idx):\n        instruction_images = []\n        for img_path in self.instruction_paths[idx]:\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            instruction_images.append(image)\n        \n        return {\n            'instructions': torch.stack(instruction_images),\n            'prompt': self.prompts[idx]\n        }","metadata":{"execution":{"iopub.status.busy":"2024-08-29T12:32:31.969304Z","iopub.execute_input":"2024-08-29T12:32:31.969795Z","iopub.status.idle":"2024-08-29T12:32:31.979736Z","shell.execute_reply.started":"2024-08-29T12:32:31.969751Z","shell.execute_reply":"2024-08-29T12:32:31.978239Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return x + self.pe[:x.size(0)]","metadata":{"execution":{"iopub.status.busy":"2024-08-29T12:32:49.546007Z","iopub.execute_input":"2024-08-29T12:32:49.546481Z","iopub.status.idle":"2024-08-29T12:32:49.557237Z","shell.execute_reply.started":"2024-08-29T12:32:49.546436Z","shell.execute_reply":"2024-08-29T12:32:49.556065Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class LEGOInstructionGenerator(nn.Module):\n    def __init__(self, num_instructions, d_model, nhead, num_encoder_layers, num_decoder_layers):\n        super().__init__()\n        \n        # Image encoder (using ResNet as feature extractor)\n        self.image_encoder = models.resnet18(pretrained=True)\n        self.image_encoder.fc = nn.Linear(self.image_encoder.fc.in_features, d_model)\n        \n        # Prompt encoder\n        self.prompt_encoder = nn.Embedding(10000, d_model)  # Assuming vocab size of 10000\n        self.positional_encoding = PositionalEncoding(d_model)\n        \n        # Transformer\n        self.transformer = nn.Transformer(\n            d_model=d_model,\n            nhead=nhead,\n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers\n        )\n        \n        # Output layer\n        self.fc_out = nn.Linear(d_model, num_instructions)\n\n    def forward(self, src_images, tgt_prompt):\n        # Encode images\n        batch_size, seq_len, c, h, w = src_images.shape\n        src_images = src_images.view(batch_size * seq_len, c, h, w)\n        src_features = self.image_encoder(src_images)\n        src_features = src_features.view(batch_size, seq_len, -1)\n        \n        # Encode prompt\n        tgt_embedded = self.prompt_encoder(tgt_prompt)\n        tgt_embedded = self.positional_encoding(tgt_embedded)\n        \n        # Transform\n        output = self.transformer(src_features, tgt_embedded)\n        \n        # Generate instruction probabilities\n        return self.fc_out(output)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T12:32:57.129902Z","iopub.execute_input":"2024-08-29T12:32:57.130511Z","iopub.status.idle":"2024-08-29T12:32:57.14227Z","shell.execute_reply.started":"2024-08-29T12:32:57.130462Z","shell.execute_reply":"2024-08-29T12:32:57.14085Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_loader, num_epochs, device):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters())\n    \n    model.to(device)\n    \n    for epoch in range(num_epochs):\n        model.train()\n        for batch in train_loader:\n            instructions = batch['instructions'].to(device)\n            prompts = batch['prompt'].to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(instructions[:, :-1], prompts)\n            loss = criterion(outputs.view(-1, outputs.size(-1)), instructions[:, 1:].view(-1))\n            loss.backward()\n            optimizer.step()\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-29T12:33:09.269361Z","iopub.execute_input":"2024-08-29T12:33:09.269849Z","iopub.status.idle":"2024-08-29T12:33:09.279699Z","shell.execute_reply.started":"2024-08-29T12:33:09.269805Z","shell.execute_reply":"2024-08-29T12:33:09.27843Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def generate_instructions(model, prompt, max_length, device):\n    model.eval()\n    with torch.no_grad():\n        prompt_tensor = torch.tensor([prompt]).to(device)\n        output_sequence = [model.prompt_encoder.weight.shape[0] - 1]  # Start token\n        \n        for _ in range(max_length):\n            tgt_tensor = torch.tensor([output_sequence]).to(device)\n            output = model(None, tgt_tensor)\n            next_item = output[0, -1].argmax().item()\n            output_sequence.append(next_item)\n            \n            if next_item == model.prompt_encoder.weight.shape[0] - 2:  # End token\n                break\n        \n    return output_sequence","metadata":{"execution":{"iopub.status.busy":"2024-08-29T12:33:10.500515Z","iopub.execute_input":"2024-08-29T12:33:10.500976Z","iopub.status.idle":"2024-08-29T12:33:10.510075Z","shell.execute_reply.started":"2024-08-29T12:33:10.50093Z","shell.execute_reply":"2024-08-29T12:33:10.508488Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Usage example\nnum_instructions = 100  # Number of possible instruction types\nd_model = 512\nnhead = 8\nnum_encoder_layers = 6\nnum_decoder_layers = 6","metadata":{"execution":{"iopub.status.busy":"2024-08-29T12:34:24.566755Z","iopub.execute_input":"2024-08-29T12:34:24.567214Z","iopub.status.idle":"2024-08-29T12:34:24.57309Z","shell.execute_reply.started":"2024-08-29T12:34:24.567171Z","shell.execute_reply":"2024-08-29T12:34:24.571776Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = LEGOInstructionGenerator(\n    num_instructions, \n    d_model, \n    nhead, \n    num_encoder_layers, \n    num_decoder_layers\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assume we have prepared our dataset\n# train_loader = DataLoader(lego_dataset, batch_size=32, shuffle=True)\n\n# Train the model\n# train_model(model, train_loader, num_epochs=10, device='cuda' if torch.cuda.is_available() else 'cpu')\n\n# Generate new instructions\n# prompt = torch.tensor([1, 2, 3, 4, 5])  # Example prompt\n# generated_instructions = generate_instructions(model, prompt, max_length=50, device='cuda' if torch.cuda.is_available() else 'cpu')\n# print(generated_instructions)","metadata":{},"execution_count":null,"outputs":[]}]}