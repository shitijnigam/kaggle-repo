{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T08:12:47.844652Z","iopub.execute_input":"2024-08-20T08:12:47.845054Z","iopub.status.idle":"2024-08-20T08:12:47.851449Z","shell.execute_reply.started":"2024-08-20T08:12:47.845019Z","shell.execute_reply":"2024-08-20T08:12:47.850399Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n# import pandas as pd\nfrom datetime import datetime\nimport time\nimport random\nimport re\nimport html","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:12:48.277901Z","iopub.execute_input":"2024-08-20T08:12:48.278921Z","iopub.status.idle":"2024-08-20T08:12:48.284372Z","shell.execute_reply.started":"2024-08-20T08:12:48.278878Z","shell.execute_reply":"2024-08-20T08:12:48.282995Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# boilerplate removal\n\ndef remove_boilerplate(text):\n    boilerplate = ['cookie policy', 'privacy policy', 'terms of service', 'all rights reserved', '\\n']\n    for phrase in boilerplate:\n        text = re.sub(r'(?i)' + re.escape(phrase) + r'.*', '', text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:12:48.909727Z","iopub.execute_input":"2024-08-20T08:12:48.910154Z","iopub.status.idle":"2024-08-20T08:12:48.919404Z","shell.execute_reply.started":"2024-08-20T08:12:48.910121Z","shell.execute_reply":"2024-08-20T08:12:48.918196Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# comment truncation (20% threshold default)\n\ndef truncate_at_comments(review_text, threshold_percentage=20, min_review_length=100):\n    comment_identifiers = [\n        'Comments', 'Leave a comment', 'Reader comments', \n        'What do you think?', 'Join the discussion', 'Add a comment',\n        'Post a comment', 'Write a comment', 'Show comments'\n    ]\n    \n    lower_text = review_text.lower()\n    text_length = len(lower_text)\n    threshold = max(int(text_length * (threshold_percentage / 100)), min_review_length)\n\n    # Check for comment identifiers\n    for identifier in comment_identifiers:\n        index = lower_text.find(identifier.lower())\n        if index != -1 and index > threshold:\n            return review_text[:index].strip()\n    \n    # If no identifiers found, try to detect comment-like structures\n    paragraphs = review_text.split('\\n\\n')\n    filtered_paragraphs = []\n    \n    for paragraph in paragraphs:\n        # Skip short paragraphs that might be comments\n        if len(paragraph) < 50:\n            continue\n        \n        # Skip paragraphs that start with common comment patterns\n        if re.match(r'^(Posted by|From|User|Anonymous|[\\d/]+:)', paragraph.strip()):\n            continue\n        \n        filtered_paragraphs.append(paragraph)\n    \n    # If we've removed some paragraphs, join the remaining ones\n    if len(filtered_paragraphs) < len(paragraphs):\n        return '\\n\\n'.join(filtered_paragraphs).strip()\n    \n    # If we haven't removed any paragraphs, return at least the first part of the text\n    return review_text[:max(threshold, len(review_text) // 2)].strip()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:12:49.388797Z","iopub.execute_input":"2024-08-20T08:12:49.389183Z","iopub.status.idle":"2024-08-20T08:12:49.398560Z","shell.execute_reply.started":"2024-08-20T08:12:49.389153Z","shell.execute_reply":"2024-08-20T08:12:49.397348Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# remove comment classes\n\ndef remove_comments(review_soup):\n    # Common class names for comment sections\n    comment_classes = ['comment', 'comments-list', 'comments-area', 'comments', 'comment-section', 'user-comments', 'disqus_thread']\n    \n    for class_name in comment_classes:\n        comment_section = review_soup.find('div', class_=class_name)\n        if comment_section:\n            comment_section.decompose()  # This removes the element from the soup\n    \n    return review_soup","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:12:49.884600Z","iopub.execute_input":"2024-08-20T08:12:49.884970Z","iopub.status.idle":"2024-08-20T08:12:49.890965Z","shell.execute_reply.started":"2024-08-20T08:12:49.884940Z","shell.execute_reply":"2024-08-20T08:12:49.889875Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# paragraph scoring for review content\n\ndef score_paragraph(paragraph):\n    review_keywords = ['review', 'book', 'read', 'author', 'story', 'character', 'plot', 'recommend']\n    return sum(keyword in paragraph.lower() for keyword in review_keywords)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:12:50.331428Z","iopub.execute_input":"2024-08-20T08:12:50.331892Z","iopub.status.idle":"2024-08-20T08:12:50.338487Z","shell.execute_reply.started":"2024-08-20T08:12:50.331857Z","shell.execute_reply":"2024-08-20T08:12:50.337062Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def process_review_url(review_url, headers):\n    reviews = []\n    # Skip known non-review sites\n    if any(site in review_url for site in ['google.com', 'wikipedia.org']):\n        return reviews\n    \n    try:\n        # Fetch the review page\n        review_response = requests.get(review_url, headers=headers, timeout=10)\n        review_soup = BeautifulSoup(review_response.text, 'html.parser')\n        \n        # Remove Comments\n        review_soup = remove_comments(review_soup)\n        \n        # Review elements\n        review_elements = review_soup.find_all(['main','p'])\n        scored_paragraphs = [(elem, score_paragraph(elem.text)) for elem in review_elements]\n        \n        # Extract review text\n        review_paragraphs = [elem.text for elem, score in sorted(scored_paragraphs, key=lambda x: x[1], reverse=True)[:3]]\n        \n        # Search for relevant text\n        review_text = ' '.join(review_paragraphs)\n        \n        # Remove boilerplate\n        review_text = review_text.replace('\\n', ' ')\n        review_text = remove_boilerplate(review_text)\n        \n        # Truncate at Comments\n        review_text = truncate_at_comments(review_text)\n        \n        review_date = 'Unknown'\n        \n        reviews.append({\n            'review_text': review_text[:5000],  # Limit to first 5000 characters\n            'review_date': review_date,\n            'review_website': review_url\n        })\n        \n    except Exception as e:\n        print(f\"Error processing {review_url}: {str(e)}\")\n    \n    # Be polite to servers\n    time.sleep(random.uniform(1, 3))\n    \n    return reviews","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:12:50.849940Z","iopub.execute_input":"2024-08-20T08:12:50.850330Z","iopub.status.idle":"2024-08-20T08:12:50.860473Z","shell.execute_reply.started":"2024-08-20T08:12:50.850300Z","shell.execute_reply":"2024-08-20T08:12:50.859062Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# search_book_review test\n\ndef search_book_reviews_api(book_name, author):\n    # Combine book name and author for search query\n    search_query = f\"{book_name} {author} book review -site:goodreads.com -site:amazon.* -site:reddit.com -site:thestorygraph.* -site:youtube.* -site:*.tv -site:barnesandnoble.com -site:wikipedia.* -site:quora.com\"\n    \n    # DuckDuckGo search API URL\n    url = f\"https://api.duckduckgo.com/?q={search_query}&format=json&pretty=1\"\n    \n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n    \n    print(f\"Searching for: {search_query}\")\n    \n    response = requests.get(url, headers=headers)\n    search_data = response.json()\n    \n    print(search_data)\n    \n    reviews = []\n    \n    # Process 'RelatedTopics' instead of 'Results'\n    for topic in search_data.get('RelatedTopics', []):\n        if 'Topics' in topic:\n            # This is a category, process its subtopics\n            for subtopic in topic['Topics']:\n                review_url = subtopic.get('FirstURL')\n                if not review_url:\n                    continue\n                reviews.extend(process_review_url(review_url, headers))\n        else:\n            review_url = topic.get('FirstURL')\n            if not review_url:\n                continue\n            reviews.extend(process_review_url(review_url, headers))\n    \n    # Create DataFrame\n    df = pd.DataFrame(reviews)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def search_book_reviews(book_name, author):\n    # Combine book name and author for search query\n    search_query = f\"{book_name} {author} book review -site:goodreads.com -site:amazon.* -site:reddit.com -site:thestorygraph.* -site:youtube.* -site:*.tv -site:barnesandnoble.com -site:wikipedia.* -site:quora.com\"\n    \n    # DuckDuckGo search URL\n    url = f\"https://html.duckduckgo.com/html/?q={requests.utils.quote(search_query)}\"\n    \n    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n    \n    print(f\"Searching for: {search_query}\")\n    \n    response = requests.get(url, headers=headers)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    reviews = []\n    \n    # Find all search result links\n    results = soup.find_all('a', class_='result__a')\n    \n    for result in results:\n        review_url = result['href']\n        \n        # Skip known non-review sites\n        if any(site in review_url for site in ['google.com', 'wikipedia.org']):\n            continue\n        \n        try:\n            # Fetch the review page\n            review_response = requests.get(review_url, headers=headers, timeout=10)\n            review_soup = BeautifulSoup(review_response.text, 'html.parser')\n            \n            # Remove Comments\n            review_soup = remove_comments(review_soup)\n            \n            # Review elements\n            review_elements = review_soup.find_all(['main','p'])\n            scored_paragraphs = [(elem, score_paragraph(elem.text)) for elem in review_elements]\n            \n            # Extract review text\n            review_paragraphs = [elem.text for elem, score in sorted(scored_paragraphs, key=lambda x: x[1], reverse=True)[:3]]\n            \n            # Search for relevant text\n            review_text = ' '.join(review_paragraphs)\n            \n            # Remove boilerplate\n            review_text = review_text.replace('\\n', ' ')\n            review_text = remove_boilerplate(review_text)\n            \n            # Truncate at Comments\n            review_text = truncate_at_comments(review_text)\n            \n            review_date = 'Unknown'\n            \n            reviews.append({\n                'review_text': review_text[:5000],  # Limit to first 5000 characters\n                'review_date': review_date,\n                'review_website': review_url\n            })\n            \n        except Exception as e:\n            print(f\"Error processing {review_url}: {str(e)}\")\n        \n        # Be polite to servers\n        time.sleep(random.uniform(1, 3))\n    \n    # Create DataFrame\n    df = pd.DataFrame(reviews)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\nbook_name = \"Darth Plagueis\"\nauthor = \"James Luceno\"\n\ndf = search_book_reviews(book_name, author)\n\nprint(df)\n\n# Save to CSV\n# df.to_csv(f\"{book_name.replace(' ', '_')}_reviews.csv\", index=False)\n# print(f\"Reviews saved to {book_name.replace(' ', '_')}_reviews.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}