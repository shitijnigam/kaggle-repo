{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-29T08:17:56.485499Z","iopub.execute_input":"2024-08-29T08:17:56.485922Z","iopub.status.idle":"2024-08-29T08:17:56.904352Z","shell.execute_reply.started":"2024-08-29T08:17:56.485876Z","shell.execute_reply":"2024-08-29T08:17:56.903120Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/book-review-samples/goodreads_fantasy.csv\n/kaggle/input/book-review-samples/goodreads_all_genres.csv\n/kaggle/input/book-review-samples/Darth_Plagueis_reviews(4).csv\n/kaggle/input/book-review-samples/goodreads_all_genres_final.csv\n/kaggle/input/book-review-samples/Dune_Book_1_reviews.csv\n/kaggle/input/book-review-samples/Dune_Book_1_reviews(4).csv\n/kaggle/input/book-review-samples/goodreads_genres_complete.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nimport concurrent.futures\nimport time\nimport os","metadata":{"execution":{"iopub.status.busy":"2024-08-29T08:18:00.054391Z","iopub.execute_input":"2024-08-29T08:18:00.054952Z","iopub.status.idle":"2024-08-29T08:18:00.393450Z","shell.execute_reply.started":"2024-08-29T08:18:00.054910Z","shell.execute_reply":"2024-08-29T08:18:00.392288Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def scrape_book_data(url):\n    try:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Extract genres\n        genres = soup.find_all('span', class_='BookPageMetadataSection__genreButton')\n        genres = ';'.join([genre.text.strip() for genre in genres])\n        \n        # Extract cover image ID\n        cover_image = soup.find('div', class_='BookCover__image')\n        cover_image_id = cover_image.find('img')['src'] if cover_image else ''\n        \n        # Extract ISBN and ASIN\n        edition_details = soup.find('div', class_='EditionDetails')\n        isbn = ''\n        asin = ''\n        if edition_details:\n            for detail in edition_details.find_all('div'):\n                if 'ISBN' in detail.text:\n                    isbn = detail.text.split(':')[-1].strip()\n                elif 'ASIN' in detail.text:\n                    asin = detail.text.split(':')[-1].strip()\n        \n        # Extract total pages\n        featured_details = soup.find('div', class_='FeaturedDetails')\n        total_pages = ''\n        if featured_details:\n            pages_info = featured_details.find(string=lambda text: 'pages' in text.lower())\n            if pages_info:\n                total_pages = pages_info.split(',')[0].strip()\n        \n        return {\n            'genres': genres,\n            'cover_image_id': cover_image_id,\n            'isbn': isbn,\n            'asin': asin,\n            'total_pages': total_pages\n        }\n    except Exception as e:\n        print(f\"Error scraping {url}: {str(e)}\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-08-29T08:18:09.185415Z","iopub.execute_input":"2024-08-29T08:18:09.185963Z","iopub.status.idle":"2024-08-29T08:18:09.196926Z","shell.execute_reply.started":"2024-08-29T08:18:09.185926Z","shell.execute_reply":"2024-08-29T08:18:09.195756Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def process_batch(batch):\n    results = []\n    for _, row in batch.iterrows():\n        data = scrape_book_data(row['URL'])\n        if data:\n            results.append({**row.to_dict(), **data})\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-08-29T08:18:45.705605Z","iopub.execute_input":"2024-08-29T08:18:45.706732Z","iopub.status.idle":"2024-08-29T08:18:45.712403Z","shell.execute_reply.started":"2024-08-29T08:18:45.706662Z","shell.execute_reply":"2024-08-29T08:18:45.711070Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"input_csv = '/kaggle/input/book-review-samples/goodreads_all_genres_final.csv'","metadata":{"execution":{"iopub.status.busy":"2024-08-29T08:18:45.895585Z","iopub.execute_input":"2024-08-29T08:18:45.896582Z","iopub.status.idle":"2024-08-29T08:18:45.900827Z","shell.execute_reply.started":"2024-08-29T08:18:45.896533Z","shell.execute_reply":"2024-08-29T08:18:45.899746Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def main():\n    # Read the original CSV\n    df = pd.read_csv(input_csv)\n    \n    # Process in batches of 10\n    batch_size = 10\n    all_results = []\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n        for i in range(0, len(df), batch_size):\n            batch = df.iloc[i:i+batch_size]\n            future = executor.submit(process_batch, batch)\n            all_results.extend(future.result())\n            \n            # Save intermediate results\n            batch_df = pd.DataFrame(all_results)\n            batch_df.to_csv(f'batch_{i//batch_size}.csv', index=False)\n            \n            print(f\"Processed {len(all_results)} books\")\n            time.sleep(1)  # To avoid overwhelming the server\n    \n    # Combine all batches into a single CSV\n    final_df = pd.DataFrame(all_results)\n    final_df.to_csv('updated_books.csv', index=False)\n    \n    # Clean up intermediate batch files\n    for file in os.listdir():\n        if file.startswith('batch_') and file.endswith('.csv'):\n            os.remove(file)","metadata":{"execution":{"iopub.status.busy":"2024-08-29T08:18:46.655579Z","iopub.execute_input":"2024-08-29T08:18:46.656425Z","iopub.status.idle":"2024-08-29T08:18:46.664411Z","shell.execute_reply.started":"2024-08-29T08:18:46.656378Z","shell.execute_reply":"2024-08-29T08:18:46.663131Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-08-29T08:18:52.507577Z","iopub.execute_input":"2024-08-29T08:18:52.507986Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Processed 10 books\nProcessed 20 books\nProcessed 30 books\nProcessed 40 books\nProcessed 50 books\nProcessed 60 books\nProcessed 70 books\nProcessed 80 books\nProcessed 90 books\nProcessed 100 books\nProcessed 110 books\nProcessed 120 books\nProcessed 130 books\nProcessed 140 books\nProcessed 150 books\nProcessed 160 books\nProcessed 170 books\nProcessed 180 books\nProcessed 190 books\nProcessed 200 books\nProcessed 210 books\nProcessed 220 books\nProcessed 230 books\nProcessed 240 books\nProcessed 250 books\nProcessed 260 books\nProcessed 270 books\nProcessed 280 books\nProcessed 290 books\nProcessed 300 books\nProcessed 310 books\nProcessed 320 books\nProcessed 330 books\nProcessed 340 books\nProcessed 350 books\nProcessed 360 books\nProcessed 370 books\nProcessed 380 books\nProcessed 390 books\nProcessed 400 books\nProcessed 410 books\nProcessed 420 books\nProcessed 430 books\nProcessed 440 books\nProcessed 450 books\nProcessed 460 books\nProcessed 470 books\nProcessed 480 books\nProcessed 490 books\nProcessed 500 books\nProcessed 510 books\nProcessed 520 books\nProcessed 530 books\nProcessed 540 books\nProcessed 550 books\nProcessed 560 books\nProcessed 570 books\nProcessed 580 books\nProcessed 590 books\nProcessed 600 books\nProcessed 610 books\nProcessed 620 books\nProcessed 630 books\nProcessed 640 books\nProcessed 650 books\nProcessed 660 books\nProcessed 670 books\nProcessed 680 books\nProcessed 690 books\nProcessed 700 books\nProcessed 710 books\nProcessed 720 books\nProcessed 730 books\nProcessed 740 books\nProcessed 750 books\nProcessed 760 books\nProcessed 770 books\nProcessed 780 books\nProcessed 790 books\nProcessed 800 books\nProcessed 810 books\nProcessed 820 books\nProcessed 830 books\nProcessed 840 books\nProcessed 850 books\nProcessed 860 books\nProcessed 870 books\nProcessed 880 books\nProcessed 890 books\nProcessed 900 books\nProcessed 910 books\nProcessed 920 books\nProcessed 930 books\nProcessed 940 books\nProcessed 950 books\nProcessed 960 books\nProcessed 970 books\nProcessed 980 books\nProcessed 990 books\nProcessed 1000 books\nProcessed 1010 books\nProcessed 1020 books\nProcessed 1030 books\nProcessed 1040 books\nProcessed 1050 books\nProcessed 1060 books\nProcessed 1070 books\nProcessed 1080 books\nProcessed 1090 books\nProcessed 1100 books\nProcessed 1110 books\nProcessed 1120 books\nProcessed 1130 books\nProcessed 1140 books\nProcessed 1150 books\nProcessed 1160 books\nProcessed 1170 books\nProcessed 1180 books\nProcessed 1190 books\nProcessed 1200 books\nProcessed 1210 books\nProcessed 1220 books\nProcessed 1230 books\nProcessed 1240 books\nProcessed 1250 books\nProcessed 1260 books\nProcessed 1270 books\nProcessed 1280 books\nProcessed 1290 books\nProcessed 1300 books\nProcessed 1310 books\nProcessed 1320 books\nProcessed 1330 books\nProcessed 1340 books\nProcessed 1350 books\nProcessed 1360 books\nProcessed 1370 books\nProcessed 1380 books\nProcessed 1390 books\nProcessed 1400 books\nProcessed 1410 books\nProcessed 1420 books\nProcessed 1430 books\nProcessed 1440 books\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}