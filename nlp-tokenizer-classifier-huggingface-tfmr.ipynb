{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":33657,"databundleVersionId":3279164,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nigamshitij/nlp-tokenizer-classifier-huggingface-tfmr?scriptVersionId=177048331\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-11T13:45:26.577359Z","iopub.execute_input":"2024-05-11T13:45:26.577627Z","iopub.status.idle":"2024-05-11T13:45:28.129964Z","shell.execute_reply.started":"2024-05-11T13:45:26.577604Z","shell.execute_reply":"2024-05-11T13:45:28.129071Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/us-patent-phrase-to-phrase-matching/sample_submission.csv\n/kaggle/input/us-patent-phrase-to-phrase-matching/train.csv\n/kaggle/input/us-patent-phrase-to-phrase-matching/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as pl\nimport pandas as pd\nimport torch\nfrom fastai.imports import *\n\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\niskaggle","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:28.131403Z","iopub.execute_input":"2024-05-11T13:45:28.131773Z","iopub.status.idle":"2024-05-11T13:45:35.108295Z","shell.execute_reply.started":"2024-05-11T13:45:28.131748Z","shell.execute_reply":"2024-05-11T13:45:35.107428Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'Interactive'"},"metadata":{}}]},{"cell_type":"code","source":"if iskaggle:\n    path = Path('/kaggle/input/us-patent-phrase-to-phrase-matching')\n    ! pip install -q datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:35.109496Z","iopub.execute_input":"2024-05-11T13:45:35.109916Z","iopub.status.idle":"2024-05-11T13:45:49.720119Z","shell.execute_reply.started":"2024-05-11T13:45:35.109891Z","shell.execute_reply":"2024-05-11T13:45:49.719063Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# !ls {path}\ndataframe = pd.read_csv(path/'train.csv')\ndataframe.rename(columns = {'score': 'label'}, inplace = True)\nprint(dataframe.head())\n# dataframe.describe(include = 'object')\n\ndataframe['input'] = 'TEXT1: ' + dataframe.context + '; TEXT2: ' + dataframe.target + '; ANC1: ' + dataframe.anchor\n# dataframe['input'].head()\ndataframe.describe(include = 'object')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:49.72292Z","iopub.execute_input":"2024-05-11T13:45:49.723321Z","iopub.status.idle":"2024-05-11T13:45:49.987509Z","shell.execute_reply.started":"2024-05-11T13:45:49.72329Z","shell.execute_reply":"2024-05-11T13:45:49.986634Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"                 id     anchor                  target context  label\n0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                      id                       anchor       target context  \\\ncount              36473                        36473        36473   36473   \nunique             36473                          733        29340     106   \ntop     8d135da0b55b8c88  component composite coating  composition     H01   \nfreq                   1                          152           24    2186   \n\n                                                          input  \ncount                                                     36473  \nunique                                                    36473  \ntop     TEXT1: B44; TEXT2: wooden substrate; ANC1: wood article  \nfreq                                                          1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n      <th>input</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>36473</td>\n      <td>36473</td>\n      <td>36473</td>\n      <td>36473</td>\n      <td>36473</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>36473</td>\n      <td>733</td>\n      <td>29340</td>\n      <td>106</td>\n      <td>36473</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>8d135da0b55b8c88</td>\n      <td>component composite coating</td>\n      <td>composition</td>\n      <td>H01</td>\n      <td>TEXT1: B44; TEXT2: wooden substrate; ANC1: wood article</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>152</td>\n      <td>24</td>\n      <td>2186</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# tokenization\nfrom datasets import Dataset, DatasetDict\ndataset = Dataset.from_pandas(dataframe)\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:49.988713Z","iopub.execute_input":"2024-05-11T13:45:49.989106Z","iopub.status.idle":"2024-05-11T13:45:51.531483Z","shell.execute_reply.started":"2024-05-11T13:45:49.989059Z","shell.execute_reply":"2024-05-11T13:45:51.530568Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'anchor', 'target', 'context', 'label', 'input'],\n    num_rows: 36473\n})"},"metadata":{}}]},{"cell_type":"code","source":"model_name = 'microsoft/deberta-v3-small'\n# https://huggingface.co/microsoft/deberta-v3-small\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:51.532603Z","iopub.execute_input":"2024-05-11T13:45:51.533017Z","iopub.status.idle":"2024-05-11T13:45:57.08444Z","shell.execute_reply.started":"2024-05-11T13:45:51.532991Z","shell.execute_reply":"2024-05-11T13:45:57.083602Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bceb7e10acee4dd5b6706104256d83e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a67b0af98c5945ffb14ff19440715900"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74c4f378af574b2f8b90f0d0590ed9ac"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.tokenize('Hello my name is Shitij')","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:57.085551Z","iopub.execute_input":"2024-05-11T13:45:57.086023Z","iopub.status.idle":"2024-05-11T13:45:57.092611Z","shell.execute_reply.started":"2024-05-11T13:45:57.085998Z","shell.execute_reply":"2024-05-11T13:45:57.091751Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['▁Hello', '▁my', '▁name', '▁is', '▁Shit', 'ij']"},"metadata":{}}]},{"cell_type":"code","source":"# numericalization\n\ndef tokenization_function(x):\n    return tokenizer(x[\"input\"])\n\ntokenization_dataset = dataset.map(tokenization_function, batched = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:57.09366Z","iopub.execute_input":"2024-05-11T13:45:57.093975Z","iopub.status.idle":"2024-05-11T13:45:58.856994Z","shell.execute_reply.started":"2024-05-11T13:45:57.093952Z","shell.execute_reply":"2024-05-11T13:45:58.856128Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49515718faa2452fbe0a580a005c34d5"}},"metadata":{}}]},{"cell_type":"code","source":"row = tokenization_dataset[33000]\nrow['input'], row['input_ids']","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:58.858214Z","iopub.execute_input":"2024-05-11T13:45:58.858523Z","iopub.status.idle":"2024-05-11T13:45:58.867017Z","shell.execute_reply.started":"2024-05-11T13:45:58.858498Z","shell.execute_reply":"2024-05-11T13:45:58.866144Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('TEXT1: G21; TEXT2: therapeutic dose; ANC1: therapeutic applications',\n [1,\n  54453,\n  435,\n  294,\n  1098,\n  2917,\n  346,\n  54453,\n  445,\n  294,\n  8068,\n  5121,\n  346,\n  23702,\n  435,\n  294,\n  8068,\n  1567,\n  2])"},"metadata":{}}]},{"cell_type":"code","source":"# tokenizer.tokenize('therapeutic')\n# tokenizer.vocab['therap']\n# tokenizer.get_vocab()['_therapeutic']\n\neval_dataframe = pd.read_csv(path/'test.csv')\neval_dataframe.rename(columns = {'score': 'label'}, inplace = True)\neval_dataframe.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:58.869791Z","iopub.execute_input":"2024-05-11T13:45:58.870065Z","iopub.status.idle":"2024-05-11T13:45:58.893782Z","shell.execute_reply.started":"2024-05-11T13:45:58.870042Z","shell.execute_reply":"2024-05-11T13:45:58.892932Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"                 id              anchor                         target context\n0  4112d61851461f60            opc drum  inorganic photoconductor drum     G02\n1  09e418c93a776564     adjust gas flow              altering gas flow     F23\n2  36baf228038e314b      lower trunnion                 lower locating     B60\n3  1f37ead645e7f0c8       cap component                  upper portion     D06\n4  71a5b6ad068d531f  neural stimulation      artificial neural network     H04","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>anchor</th>\n      <th>target</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4112d61851461f60</td>\n      <td>opc drum</td>\n      <td>inorganic photoconductor drum</td>\n      <td>G02</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>09e418c93a776564</td>\n      <td>adjust gas flow</td>\n      <td>altering gas flow</td>\n      <td>F23</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36baf228038e314b</td>\n      <td>lower trunnion</td>\n      <td>lower locating</td>\n      <td>B60</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1f37ead645e7f0c8</td>\n      <td>cap component</td>\n      <td>upper portion</td>\n      <td>D06</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>71a5b6ad068d531f</td>\n      <td>neural stimulation</td>\n      <td>artificial neural network</td>\n      <td>H04</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset_dict = tokenization_dataset.train_test_split(0.25, seed = 42)\ndataset_dict","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:58.894854Z","iopub.execute_input":"2024-05-11T13:45:58.895196Z","iopub.status.idle":"2024-05-11T13:45:58.923605Z","shell.execute_reply.started":"2024-05-11T13:45:58.895166Z","shell.execute_reply":"2024-05-11T13:45:58.922822Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 27354\n    })\n    test: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'label', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9119\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# define correlation coefficient\ndef corr(x, y):\n    return np.corrcoef(x, y)[0][1]\n\ndef corr_dictionary(eval_pred):\n    return {'pearson': corr(*eval_pred)}","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:58.924534Z","iopub.execute_input":"2024-05-11T13:45:58.924784Z","iopub.status.idle":"2024-05-11T13:45:58.929563Z","shell.execute_reply.started":"2024-05-11T13:45:58.924763Z","shell.execute_reply":"2024-05-11T13:45:58.928632Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nbatch_size = 128 # larger, more in parallel, out of memory if too big\nepochs = 4\nlearning_rate = 8e-5","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:45:58.930572Z","iopub.execute_input":"2024-05-11T13:45:58.93089Z","iopub.status.idle":"2024-05-11T13:46:16.394181Z","shell.execute_reply.started":"2024-05-11T13:45:58.930867Z","shell.execute_reply":"2024-05-11T13:46:16.393405Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2024-05-11 13:46:04.808486: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-11 13:46:04.808586: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-11 13:46:05.078428: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"arguments = TrainingArguments(output_dir = 'outputs',\n                              learning_rate = learning_rate,\n                              warmup_ratio = 0.1,\n                              lr_scheduler_type = 'cosine',\n#                               fp16 = True,\n                              evaluation_strategy = 'epoch',\n                              per_device_train_batch_size = batch_size,\n                              per_device_eval_batch_size = batch_size*2,\n                              num_train_epochs = epochs,\n                              weight_decay = 0.01,\n                              report_to = 'none'\n                             )\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 1)\ntrainer = Trainer(model,\n                  arguments,\n                  train_dataset = dataset_dict['train'],\n                  eval_dataset = dataset_dict['test'],\n                  tokenizer = tokenizer,\n                  compute_metrics = corr_dictionary\n                 )","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:46:16.39523Z","iopub.execute_input":"2024-05-11T13:46:16.395797Z","iopub.status.idle":"2024-05-11T13:46:19.511469Z","shell.execute_reply.started":"2024-05-11T13:46:16.39577Z","shell.execute_reply":"2024-05-11T13:46:19.510655Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/286M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35fc1c10e96a4712befa7bd0ab6c8366"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# TrainingArguments??\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:46:19.512613Z","iopub.execute_input":"2024-05-11T13:46:19.512972Z","iopub.status.idle":"2024-05-11T13:50:50.440597Z","shell.execute_reply.started":"2024-05-11T13:46:19.512938Z","shell.execute_reply":"2024-05-11T13:50:50.439786Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='428' max='428' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [428/428 04:26, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Pearson</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.030553</td>\n      <td>0.788216</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>0.025993</td>\n      <td>0.817216</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>0.025985</td>\n      <td>0.825833</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>No log</td>\n      <td>0.024476</td>\n      <td>0.827091</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=428, training_loss=0.03380305967598318, metrics={'train_runtime': 270.6332, 'train_samples_per_second': 404.296, 'train_steps_per_second': 1.581, 'total_flos': 748113626405160.0, 'train_loss': 0.03380305967598318, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"# add input field for tokenization\neval_dataframe['input'] = 'TEXT1: ' + eval_dataframe.context + '; TEXT2: ' + eval_dataframe.target + '; ANC1: ' + eval_dataframe.anchor\n\n# convert dataframe to dataset\neval_dataset = Dataset.from_pandas(eval_dataframe)\n\n# map dataset to tokens using tokenization function\neval_dataset = eval_dataset.map(tokenization_function, batched = True)\n\n# run predictions on dataset using trained model (needs to have input fields)\npredictions = trainer.predict(eval_dataset).predictions.astype(float)\n\n# print predictions\npredictions","metadata":{"execution":{"iopub.status.busy":"2024-05-11T13:50:50.441926Z","iopub.execute_input":"2024-05-11T13:50:50.442599Z","iopub.status.idle":"2024-05-11T13:50:50.635682Z","shell.execute_reply.started":"2024-05-11T13:50:50.442563Z","shell.execute_reply":"2024-05-11T13:50:50.63485Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17f792d79fff4fc68d495b8b2ec50179"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"array([[ 0.55404937],\n       [ 0.71638495],\n       [ 0.51819497],\n       [ 0.36538407],\n       [-0.02204065],\n       [ 0.51772523],\n       [ 0.48330551],\n       [ 0.10196796],\n       [ 0.2809881 ],\n       [ 1.10844219],\n       [ 0.30006972],\n       [ 0.31430319],\n       [ 0.80609947],\n       [ 0.82189167],\n       [ 0.79629403],\n       [ 0.3811202 ],\n       [ 0.31546891],\n       [ 0.00935643],\n       [ 0.64860564],\n       [ 0.37407187],\n       [ 0.47903419],\n       [ 0.25257939],\n       [ 0.11335076],\n       [ 0.24104178],\n       [ 0.59749633],\n       [-0.03576969],\n       [-0.04324893],\n       [-0.02253797],\n       [-0.05716192],\n       [ 0.66999573],\n       [ 0.35355464],\n       [ 0.06568331],\n       [ 0.71808344],\n       [ 0.46307623],\n       [ 0.47121704],\n       [ 0.19375861]])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}